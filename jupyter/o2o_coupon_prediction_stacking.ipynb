{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "continous = [\n",
    "    'Discount',\n",
    "    'Previous_duration',\n",
    "    'Next_duration',\n",
    "    'Base_consume',\n",
    "    'User_receive_count',\n",
    "    'User_consume_count',\n",
    "    'User_used_count',\n",
    "    'User_not_used_count',\n",
    "    'User_used_coupon_rate',\n",
    "    'User_used_coupon_rate_max',\n",
    "    'User_used_coupon_rate_min',\n",
    "    'User_used_coupon_rate_mean',\n",
    "    'User_receive_coupon_merchant_count',\n",
    "    'User_consume_merchant_count',\n",
    "    'User_used_coupon_merchant_count',\n",
    "    'User_used_coupon_merchant_occ',\n",
    "    'User_receive_different_coupon_count',\n",
    "    'User_used_different_coupon_count',\n",
    "    'User_receive_different_coupon_occ',\n",
    "    'User_used_different_coupon_occ',\n",
    "    'User_receive_coupon_mean',\n",
    "    'User_used_coupon_mean',\n",
    "    'User_distance_used_mean',\n",
    "    'User_distance_used_max',\n",
    "    'User_distance_used_min',\n",
    "    'User_duration_used_mean',\n",
    "    'User_duration_used_max',\n",
    "    'User_duration_used_min',\n",
    "    'User_previous_duration_used_mean',\n",
    "    'User_previous_duration_used_max',\n",
    "    'User_previous_duration_used_min',\n",
    "    'User_next_duration_used_mean',\n",
    "    'User_next_duration_used_max',\n",
    "    'User_next_duration_used_min',\n",
    "    'Merchant_receive_count',\n",
    "    'Merchant_consume_count',\n",
    "    'Merchant_used_count',\n",
    "    'Merchant_not_used_count',\n",
    "    'Merchant_used_coupon_rate',\n",
    "    'Merchant_used_coupon_rate_max',\n",
    "    'Merchant_used_coupon_rate_min',\n",
    "    'Merchant_used_coupon_rate_mean',\n",
    "    'Merchant_receive_coupon_user_count',\n",
    "    'Merchant_consume_user_count',\n",
    "    'Merchant_used_coupon_user_count',\n",
    "    'Merchant_receive_coupon_user_occ',\n",
    "    'Merchant_consume_user_occ',\n",
    "    'Merchant_used_coupon_user_occ',\n",
    "    'Merchant_receive_different_coupon_count',\n",
    "    'Merchant_used_different_coupon_count',\n",
    "    'Merchant_receive_different_coupon_occ',\n",
    "    'Merchant_used_different_coupon_occ',\n",
    "    'Merchant_receive_coupon_mean',\n",
    "    'Merchant_used_coupon_mean',\n",
    "    'Merchant_receive_different_coupon_avg',\n",
    "    'Merchant_used_different_coupon_avg',\n",
    "    'Merchant_distance_used_mean',\n",
    "    'Merchant_distance_used_max',\n",
    "    'Merchant_distance_used_min',\n",
    "    'Merchant_duration_used_mean',\n",
    "    'Merchant_duration_used_max',\n",
    "    'Merchant_duration_used_min',\n",
    "    'Merchant_previous_duration_used_mean',\n",
    "    'Merchant_previous_duration_used_max',\n",
    "    'Merchant_previous_duration_used_min',\n",
    "    'Merchant_next_duration_used_mean',\n",
    "    'Merchant_next_duration_used_max',\n",
    "    'Merchant_next_duration_used_min',\n",
    "    'Coupon_received_count',\n",
    "    'Coupon_used_count',\n",
    "    'Coupon_used_rate',\n",
    "    'Coupon_duration_used_mean',\n",
    "    'Coupon_duration_used_max',\n",
    "    'Coupon_duration_used_min',\n",
    "    'Coupon_distance_used_mean',\n",
    "    'Coupon_distance_used_max',\n",
    "    'Coupon_distance_used_min',\n",
    "    'User_merchant_receive_count',\n",
    "    'User_merchant_consume_count',\n",
    "    'User_merchant_used_count',\n",
    "    'User_merchant_not_used_count',\n",
    "    'User_merchant_used_coupon_rate',\n",
    "    'User_merchant_not_used_coupon_rate',\n",
    "    'User_merchant_used_coupon_rate_4_merchant',\n",
    "    'User_merchant_not_used_coupon_rate_4_merchant',\n",
    "    'User_merchant_duration_used_mean',\n",
    "    'User_merchant_duration_used_max',\n",
    "    'User_merchant_duration_used_min',\n",
    "    'Online_user_receive_count',\n",
    "    'Online_user_consume_count',\n",
    "    'Online_user_used_count',\n",
    "    'Online_user_not_used_count',\n",
    "    'Online_user_used_coupon_rate',\n",
    "    'User_offline_consume_rate',\n",
    "    'User_offline_used_rate',\n",
    "    'User_offline_no_consume_coupon_rate',\n",
    "    'User_distance_receive_count',\n",
    "    'User_distance_consume_count',\n",
    "    'User_distance_used_count',\n",
    "    'User_distance_receive_rate',\n",
    "    'User_distance_consume_rate',\n",
    "    'User_distance_used_rate',\n",
    "    'User_coupon_type_receive_count',\n",
    "    'User_coupon_type_used_count',\n",
    "    'User_coupon_type_receive_rate',\n",
    "    'User_coupon_type_used_rate',\n",
    "    'User_coupon_receive_count',\n",
    "    'User_coupon_used_count',\n",
    "    'User_coupon_receive_rate',\n",
    "    'User_coupon_used_rate',\n",
    "    'Merchant_distance_receive_count',\n",
    "    'Merchant_distance_consume_count',\n",
    "    'Merchant_distance_used_count',\n",
    "    'Merchant_distance_receive_rate',\n",
    "    'Merchant_distance_used_rate',\n",
    "    'User_coupon_duration_used_mean',\n",
    "    'User_coupon_duration_used_max',\n",
    "    'User_coupon_duration_used_min',\n",
    "    'User_received_date_count'\n",
    "]\n",
    "\n",
    "\n",
    "fields = [\n",
    "    'Distance',\n",
    "    'Day_in_month_received',\n",
    "    'Day_in_week_received',\n",
    "    'Coupon_type'\n",
    "]\n",
    "\n",
    "label = ['Is_in_day_consume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train_df = pd.read_csv('lcm_train_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test_df = pd.read_csv('lcm_train_test_features.csv')\n",
    "model_test_df = model_test_df[model_test_df['Coupon_id']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer, MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "from mlxtend.preprocessing import DenseTransformer\n",
    "\n",
    "class ModelTransformer(TransformerMixin):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def fit(self, *args, **kwargs):\n",
    "        self.model.fit(*args, **kwargs)\n",
    "        return self\n",
    "    def transform(self, X, **transform_params):\n",
    "        return pd.DataFrame(self.model.predict(X))\n",
    "\n",
    "\n",
    "class GBDTTransformer(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.n_estimator = 256\n",
    "        self.model = GradientBoostingClassifier(max_depth=3, n_estimators=self.n_estimator, random_state=0)\n",
    "        \n",
    "    def fit(self, *args, **kwargs):\n",
    "        self.model.fit(*args, **kwargs)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        return self.model.apply(X)[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leewind/.local/share/virtualenvs/leewind-p6XO93Th/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/leewind/.local/share/virtualenvs/leewind-p6XO93Th/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('continuous', Pipeline(memory=None,\n",
       "     steps=[('extract', ColumnSelector(cols=['Discount', 'Previous_duration', 'Next_duration', 'Base_consume', 'User_receive_count', 'User_consume_count', 'User_used_count', 'User_not_used_cou...ki',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform'))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_stacking_model = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('continuous', Pipeline([\n",
    "            ('extract', ColumnSelector(continous)),\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "            ('scale', Normalizer())\n",
    "        ])),\n",
    "        ('fields', Pipeline([\n",
    "            ('extract', ColumnSelector(fields)),\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan,  strategy='most_frequent')),\n",
    "            ('one_hot', OneHotEncoder(categories='auto')),\n",
    "            ('to_dense', DenseTransformer())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('pca', PCA(n_components=64)),\n",
    "    ('estimators', FeatureUnion([\n",
    "        ('knn', ModelTransformer(KNeighborsClassifier())),\n",
    "        ('gdtc', ModelTransformer(GradientBoostingClassifier(max_depth=3, n_estimators=10, random_state=0))),\n",
    "        ('dtc', ModelTransformer(DecisionTreeClassifier())),\n",
    "        ('etc', ModelTransformer(ExtraTreeClassifier())),\n",
    "        ('rfc', ModelTransformer(RandomForestClassifier())),\n",
    "        ('pac', ModelTransformer(PassiveAggressiveClassifier())),\n",
    "        ('lr', ModelTransformer(LogisticRegression(C=0.1, random_state=2, solver='lbfgs', class_weight='balanced', multi_class='multinomial', max_iter=5000, n_jobs=4))),\n",
    "        ('cluster', ModelTransformer(KMeans(n_clusters=2)))\n",
    "    ])),\n",
    "    ('estimator', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "pipe_stacking_model.fit(model_train_df[fields+continous], model_train_df[label].values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def evaluate(result_df):\n",
    "    group = result_df.groupby(['Coupon_id'])\n",
    "    aucs = []\n",
    "    for i in group:\n",
    "        tmpdf = i[1]        \n",
    "        if len(tmpdf['Is_in_day_consume'].unique()) != 2:\n",
    "            continue\n",
    "            \n",
    "        fpr, tpr, thresholds = roc_curve(tmpdf['Is_in_day_consume'], tmpdf['Probability'], pos_label=1)\n",
    "        auc_score = auc(fpr,tpr)\n",
    "        aucs.append(auc_score)\n",
    "            \n",
    "    return np.average(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test_prob_y = pipe_stacking_model.predict_proba(model_test_df[fields+continous])\n",
    "model_test_df['Probability'] = predict_test_prob_y[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5178096740396808"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred_df = pd.read_csv('lcm_test_features.csv')\n",
    "predict_prob_y = pipe_stacking_model.predict_proba(model_pred_df[fields+continous])\n",
    "model_pred_df['Probability'] = predict_prob_y[:, 1]\n",
    "model_pred_df.sort_values(['Probability'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_df = model_pred_df[['User_id', 'Coupon_id', 'Date_received', 'Probability']]\n",
    "final_result_df.to_csv('/Users/leewind/Desktop/submission_20190118.csv', index=False, header=False)\n",
    "final_result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
