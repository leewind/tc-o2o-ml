{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import math\n",
    "import logging\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder,MinMaxScaler, Normalizer\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from mlxtend.preprocessing import DenseTransformer\n",
    "from mlxtend.feature_selection import ColumnSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('ai')\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(asctime)s  %(filename)s : %(levelname)s  %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_alpha = pd.read_csv('../features/dataset_alpha.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_beta = pd.read_csv('../features/dataset_beta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "continous = ['Distance',\n",
    " 'Base_consume',\n",
    " 'Discount_money',\n",
    " 'Next_duration',\n",
    " 'Receive_coupon_category',\n",
    " 'Receive_coupon_category_rate',\n",
    " 'Receive_coupon_id',\n",
    " 'Avg_user_per_coupon_count',\n",
    " 'Avg_user_per_merchant_count',\n",
    " 'Receive_merchant_id',\n",
    " 'Receive_base_consume',\n",
    " 'Coupon_id_receive',\n",
    " 'Merchant_id_receive',\n",
    " 'User_discount_mean',\n",
    " 'Receive_coupon_type_rate',\n",
    " 'Receive_weekday_of_received_rate',\n",
    " 'Collect_in_day_rate',\n",
    " 'Avg_user_receive_coupon',\n",
    " 'Avg_user_receive_merchant',\n",
    " 'H_Use_duration_mean_U',\n",
    " 'H_Use_UCC',\n",
    " 'H_Use_UC_rate',\n",
    " 'H_Receive_M',\n",
    " 'H_Consume_M',\n",
    " 'H_Use_M',\n",
    " 'H_No_use_M',\n",
    " 'H_Use_occ_M',\n",
    " 'H_Consume_duration_mean_M',\n",
    " 'H_Use_duration_mean_M',\n",
    " 'H_Receive_C',\n",
    " 'H_Consume_C',\n",
    " 'H_Use_C',\n",
    " 'H_O_Receive_U',\n",
    " 'H_Consume_duration_mean_UCC',\n",
    " 'User_discount_max',\n",
    " 'User_discount_min']\n",
    "\n",
    "label = ['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = pd.read_csv('/Users/leewind/Projects/leewind/tianchi_O2O_predict/data_preprocessed_2/ProcessDataSet1.csv')\n",
    "dataset_1.drop_duplicates(inplace=True)\n",
    "dataset_1.fillna(0, inplace=True)\n",
    "dataset_1_x = dataset_1.drop(\n",
    "        columns=['User_id', 'Merchant_id', 'Discount_rate', 'Date_received', 'discount_rate_x', 'discount_rate_y',\n",
    "                 'Date', 'Coupon_id', 'label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2 = pd.read_csv('/Users/leewind/Projects/leewind/tianchi_O2O_predict/data_preprocessed_2/ProcessDataSet2.csv')\n",
    "dataset_2.drop_duplicates(inplace=True)\n",
    "dataset_2.fillna(0, inplace=True)\n",
    "dataset_2_x = dataset_2.drop(\n",
    "        columns=['User_id', 'Merchant_id', 'Discount_rate', 'Date_received', 'discount_rate_x', 'discount_rate_y',\n",
    "                 'Date', 'Coupon_id', 'label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "continous = [\n",
    "    'Distance', 'discount_rate', 'weekday', 'day', 'u2', 'u3', 'u19',\n",
    "       'u1', 'u4', 'u5', 'u25', 'u20', 'u6', 'u7', 'u8', 'u9', 'u10',\n",
    "       'u11', 'u21', 'u22', 'u23', 'u24', 'u45', 'u27', 'u28', 'u32',\n",
    "       'u47', 'u33', 'u34', 'u35', 'u36', 'u37', 'discount_type', 'u41',\n",
    "       'u42', 'u43', 'u44', 'u48', 'u49', 'm0', 'm1', 'm2', 'm3', 'm4',\n",
    "       'm7', 'm5', 'm6', 'm8', 'm9', 'm10', 'm11', 'm12', 'm13', 'm14',\n",
    "       'm15', 'm18', 'm19', 'm20', 'm21', 'm22', 'm23', 'c1', 'c2', 'c3',\n",
    "       'c4', 'c5', 'c6', 'c8', 'c9', 'c10', 'c11', 'c12', 'um1', 'um2',\n",
    "       'um3', 'um4', 'um5', 'um6', 'um7', 'um8', 'um9', 'um10', 'um11',\n",
    "       'um12', 'o1', 'o2', 'o17', 'o18', 'o3', 'o4', 'o5', 'o6', 'o7',\n",
    "       'o8', 'o9', 'o10', 'o11', 'o12', 'o13', 'o14', 'o15', 'o16',\n",
    "       'on_u1', 'on_u2', 'on_u3', 'on_u4', 'on_u5', 'on_u6', 'on_u7',\n",
    "       'on_u8', 'on_u9', 'on_u10', 'on_u11', 'on_u12', 'on_u13'\n",
    "]\n",
    "\n",
    "fields = [\n",
    "\n",
    "]\n",
    "\n",
    "label = ['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeFeature(TransformerMixin):\n",
    "    def __init__(self, df, key, prefix, pipe):\n",
    "        self.df = df\n",
    "        self.key = key\n",
    "        self.prefix = prefix\n",
    "        self.pipe = pipe\n",
    "    \n",
    "    def get_factor(self, df, key, prefix):\n",
    "        id_df = df[[key]]\n",
    "        output_df = df.drop([key], axis=1)\n",
    "\n",
    "        self.pipe.fit(output_df)\n",
    "        factors = self.pipe.transform(output_df)\n",
    "        factors_df = pd.DataFrame(data=factors, columns=[prefix + '_factor_alpha', prefix + '_factor_beta'])\n",
    "        factors_df[key] = id_df[key]\n",
    "        return factors_df\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        return pd.merge(X, self.df, on=self.key, how='left')\n",
    "#         return pd.merge(X, self.get_factor(self.df, self.key, self.prefix), on=[self.key], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBDTTransformer(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.n_estimator = 256\n",
    "        self.model = GradientBoostingClassifier(max_depth=3, n_estimators=self.n_estimator, random_state=0)\n",
    "        \n",
    "    def fit(self, *args, **kwargs):\n",
    "        self.model.fit(*args, **kwargs)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        return self.model.apply(X)[:, :, 0]\n",
    "    \n",
    "class ExtractFeature(TransformerMixin):\n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        return pd.DataFrame(X[:,0] * X[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-05 11:05:21,499  <ipython-input-8-098a8269e384> : INFO  Start training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('continuous', Pipeline(memory=None,\n",
       "     steps=[('extract', ColumnSelector(cols=['Distance', 'Base_consume', 'Discount_money', 'Next_duration', 'Receive_coupon_category', 'Receive_coupon_category_rate', 'Receive_coupon_id', 'Avg...'l2',\n",
       "          random_state=2, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "          warm_start=False))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factor_pipeline = Pipeline([\n",
    "    ('pca', PCA(n_components=2)),\n",
    "    ('scale', MinMaxScaler()),\n",
    "])\n",
    "\n",
    "pipe_lr = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('continuous', Pipeline([\n",
    "            ('extract', ColumnSelector(continous)),\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "#             ('scale', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
    "            ('normalize', Normalizer())\n",
    "        ])),\n",
    "#         ('fields', Pipeline([\n",
    "#             ('extract', ColumnSelector(fields)),\n",
    "#             ('imputer', SimpleImputer(missing_values=np.nan,  strategy='most_frequent')),\n",
    "#             ('one_hot', OneHotEncoder(categories='auto')),\n",
    "#             ('to_dense', DenseTransformer())\n",
    "#         ])),\n",
    "    ])),\n",
    "#     ('skb', SelectKBest(chi2)),\n",
    "    ('gbdt', GBDTTransformer()),\n",
    "    ('pca', PCA()),\n",
    "    ('clf', LogisticRegression(C=0.1, random_state=2, solver='lbfgs', class_weight='balanced', multi_class='multinomial', max_iter=5000, n_jobs=4))\n",
    "])\n",
    "\n",
    "logger.info('Start training')\n",
    "pipe_lr.set_params(\n",
    "    pca__n_components=8, \n",
    "#     skb__k=32\n",
    ").fit(dataset_beta, dataset_beta['Label'].values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def evaluate(result_df):\n",
    "    group = result_df.groupby(['Coupon_id'])\n",
    "    aucs = []\n",
    "    for i in group:\n",
    "        tmpdf = i[1]        \n",
    "        if len(tmpdf['Label'].unique()) != 2:\n",
    "            continue\n",
    "            \n",
    "        fpr, tpr, thresholds = roc_curve(tmpdf['Label'], tmpdf['Prob'], pos_label=1)\n",
    "        auc_score = auc(fpr,tpr)\n",
    "        aucs.append(auc_score)\n",
    "            \n",
    "    return np.average(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test_prob_y = pipe_lr.predict_proba(dataset_alpha)\n",
    "dataset_alpha['Prob'] = predict_test_prob_y[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6551828585411936"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(dataset_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred_df = pd.read_csv('lcm_test_features.csv')\n",
    "predict_prob_y = pipe_lr.predict_proba(model_pred_df[fields+continous])\n",
    "model_pred_df['Probability'] = predict_prob_y[:, 1]\n",
    "model_pred_df.sort_values(['Probability'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_df = model_pred_df[['User_id', 'Coupon_id', 'Date_received', 'Probability']]\n",
    "final_result_df.to_csv('/Users/leewind/Desktop/submission_20190118.csv', index=False, header=False)\n",
    "final_result_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
