{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "+ [Feature transformations with ensembles of trees](https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html)\n",
    "+ [sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "+ [机器学习之 sklearn中的pipeline](http://frankchen.xyz/2018/04/08/pipeline-in-machine-learning/)\n",
    "    - 使用pipeline做cross validation\n",
    "    - 自定义transformer\n",
    "    - FeatureUnion\n",
    "+ [Concatenating multiple feature extraction methods](https://scikit-learn.org/stable/auto_examples/compose/plot_feature_union.html#sphx-glr-auto-examples-compose-plot-feature-union-py)\n",
    "+ [sklearn.pipeline.Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "+ [gbdt+lr demo](https://github.com/princewen/tensorflow_practice/blob/master/recommendation/GBDT%2BLR-Demo/GBDT_LR.py)\n",
    "+ [推荐系统遇上深度学习(十)--GBDT+LR融合方案实战](https://zhuanlan.zhihu.com/p/37522339)\n",
    "+ [python︱sklearn一些小技巧的记录（训练集划分/pipelline/交叉验证等）](https://blog.csdn.net/sinat_26917383/article/details/77917881)\n",
    "+ [16.【进阶】特征提升之特征筛选----feature_selection](https://blog.csdn.net/jh1137921986/article/details/79822512)\n",
    "+ [使用sklearn优雅地进行数据挖掘](https://www.cnblogs.com/jasonfreak/p/5448462.html)\n",
    "+ [Kaggle机器学习之模型融合（stacking）心得](https://zhuanlan.zhihu.com/p/26890738)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "continous = [\n",
    "    'Discount',\n",
    "    'Previous_duration',\n",
    "    'Next_duration',\n",
    "    'Base_consume',\n",
    "    'User_receive_count',\n",
    "    'User_consume_count',\n",
    "    'User_used_count',\n",
    "    'User_not_used_count',\n",
    "    'User_used_coupon_rate',\n",
    "    'User_used_coupon_rate_max',\n",
    "    'User_used_coupon_rate_min',\n",
    "    'User_used_coupon_rate_mean',\n",
    "    'User_receive_coupon_merchant_count',\n",
    "    'User_consume_merchant_count',\n",
    "    'User_used_coupon_merchant_count',\n",
    "    'User_used_coupon_merchant_occ',\n",
    "    'User_receive_different_coupon_count',\n",
    "    'User_used_different_coupon_count',\n",
    "    'User_receive_different_coupon_occ',\n",
    "    'User_used_different_coupon_occ',\n",
    "    'User_receive_coupon_mean',\n",
    "    'User_used_coupon_mean',\n",
    "    'User_distance_used_mean',\n",
    "    'User_distance_used_max',\n",
    "    'User_distance_used_min',\n",
    "    'User_duration_used_mean',\n",
    "    'User_duration_used_max',\n",
    "    'User_duration_used_min',\n",
    "    'User_previous_duration_used_mean',\n",
    "    'User_previous_duration_used_max',\n",
    "    'User_previous_duration_used_min',\n",
    "    'User_next_duration_used_mean',\n",
    "    'User_next_duration_used_max',\n",
    "    'User_next_duration_used_min',\n",
    "    'Merchant_receive_count',\n",
    "    'Merchant_consume_count',\n",
    "    'Merchant_used_count',\n",
    "    'Merchant_not_used_count',\n",
    "    'Merchant_used_coupon_rate',\n",
    "    'Merchant_used_coupon_rate_max',\n",
    "    'Merchant_used_coupon_rate_min',\n",
    "    'Merchant_used_coupon_rate_mean',\n",
    "    'Merchant_receive_coupon_user_count',\n",
    "    'Merchant_consume_user_count',\n",
    "    'Merchant_used_coupon_user_count',\n",
    "    'Merchant_receive_coupon_user_occ',\n",
    "    'Merchant_consume_user_occ',\n",
    "    'Merchant_used_coupon_user_occ',\n",
    "    'Merchant_receive_different_coupon_count',\n",
    "    'Merchant_used_different_coupon_count',\n",
    "    'Merchant_receive_different_coupon_occ',\n",
    "    'Merchant_used_different_coupon_occ',\n",
    "    'Merchant_receive_coupon_mean',\n",
    "    'Merchant_used_coupon_mean',\n",
    "    'Merchant_receive_different_coupon_avg',\n",
    "    'Merchant_used_different_coupon_avg',\n",
    "    'Merchant_distance_used_mean',\n",
    "    'Merchant_distance_used_max',\n",
    "    'Merchant_distance_used_min',\n",
    "    'Merchant_duration_used_mean',\n",
    "    'Merchant_duration_used_max',\n",
    "    'Merchant_duration_used_min',\n",
    "    'Merchant_previous_duration_used_mean',\n",
    "    'Merchant_previous_duration_used_max',\n",
    "    'Merchant_previous_duration_used_min',\n",
    "    'Merchant_next_duration_used_mean',\n",
    "    'Merchant_next_duration_used_max',\n",
    "    'Merchant_next_duration_used_min',\n",
    "    'Coupon_received_count',\n",
    "    'Coupon_used_count',\n",
    "    'Coupon_used_rate',\n",
    "    'Coupon_duration_used_mean',\n",
    "    'Coupon_duration_used_max',\n",
    "    'Coupon_duration_used_min',\n",
    "    'Coupon_distance_used_mean',\n",
    "    'Coupon_distance_used_max',\n",
    "    'Coupon_distance_used_min',\n",
    "    'User_merchant_receive_count',\n",
    "    'User_merchant_consume_count',\n",
    "    'User_merchant_used_count',\n",
    "    'User_merchant_not_used_count',\n",
    "    'User_merchant_used_coupon_rate',\n",
    "    'User_merchant_not_used_coupon_rate',\n",
    "    'User_merchant_used_coupon_rate_4_merchant',\n",
    "    'User_merchant_not_used_coupon_rate_4_merchant',\n",
    "    'User_merchant_duration_used_mean',\n",
    "    'User_merchant_duration_used_max',\n",
    "    'User_merchant_duration_used_min',\n",
    "    'Online_user_receive_count',\n",
    "    'Online_user_consume_count',\n",
    "    'Online_user_used_count',\n",
    "    'Online_user_not_used_count',\n",
    "    'Online_user_used_coupon_rate',\n",
    "    'User_offline_consume_rate',\n",
    "    'User_offline_used_rate',\n",
    "    'User_offline_no_consume_coupon_rate',\n",
    "    'User_distance_receive_count',\n",
    "    'User_distance_consume_count',\n",
    "    'User_distance_used_count',\n",
    "    'User_distance_receive_rate',\n",
    "    'User_distance_consume_rate',\n",
    "    'User_distance_used_rate',\n",
    "    'User_coupon_type_receive_count',\n",
    "    'User_coupon_type_used_count',\n",
    "    'User_coupon_type_receive_rate',\n",
    "    'User_coupon_type_used_rate',\n",
    "    'User_coupon_receive_count',\n",
    "    'User_coupon_used_count',\n",
    "    'User_coupon_receive_rate',\n",
    "    'User_coupon_used_rate',\n",
    "    'Merchant_distance_receive_count',\n",
    "    'Merchant_distance_consume_count',\n",
    "    'Merchant_distance_used_count',\n",
    "    'Merchant_distance_receive_rate',\n",
    "    'Merchant_distance_used_rate',\n",
    "    'User_coupon_duration_used_mean',\n",
    "    'User_coupon_duration_used_max',\n",
    "    'User_coupon_duration_used_min',\n",
    "    'User_received_date_count'\n",
    "]\n",
    "\n",
    "\n",
    "fields = [\n",
    "    'Distance',\n",
    "    'Day_in_month_received',\n",
    "    'Day_in_week_received',\n",
    "    'Coupon_type'\n",
    "]\n",
    "\n",
    "label = ['Is_in_day_consume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'lcm_train_features.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4863c9518eee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_train_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lcm_train_features.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/leewind-p6XO93Th/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/leewind-p6XO93Th/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/leewind-p6XO93Th/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/leewind-p6XO93Th/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/leewind-p6XO93Th/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'lcm_train_features.csv' does not exist"
     ]
    }
   ],
   "source": [
    "model_train_df = pd.read_csv('lcm_train_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test_df = pd.read_csv('lcm_train_test_features.csv')\n",
    "model_test_df = model_test_df[model_test_df['Coupon_id']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class GBDTTransformer(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.n_estimator = 256\n",
    "        self.model = GradientBoostingClassifier(max_depth=3, n_estimators=self.n_estimator, random_state=0)\n",
    "        \n",
    "    def fit(self, *args, **kwargs):\n",
    "        self.model.fit(*args, **kwargs)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        return self.model.apply(X)[:, :, 0]\n",
    "    \n",
    "class ExtractFeature(TransformerMixin):\n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        return pd.DataFrame(X[:,0] * X[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('continuous', Pipeline(memory=None,\n",
       "     steps=[('extract', ColumnSelector(cols=['Discount', 'Previous_duration', 'Next_duration', 'Base_consume', 'User_receive_count', 'User_consume_count', 'User_used_count', 'User_not_used_cou...'l2',\n",
       "          random_state=2, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "          warm_start=False))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer\n",
    "from mlxtend.preprocessing import DenseTransformer\n",
    "\n",
    "pipe_lr = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('continuous', Pipeline([\n",
    "            ('extract', ColumnSelector(continous)),\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "            ('scale', Normalizer())\n",
    "        ])),\n",
    "        ('fields', Pipeline([\n",
    "            ('extract', ColumnSelector(fields)),\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan,  strategy='most_frequent')),\n",
    "            ('one_hot', OneHotEncoder(categories='auto')),\n",
    "            ('to_dense', DenseTransformer())\n",
    "        ])),\n",
    "        ('rate', Pipeline([\n",
    "            ('extract', ColumnSelector(['User_coupon_used_rate', 'User_used_coupon_rate'])),\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan,  strategy='most_frequent')),\n",
    "            ('new', ExtractFeature()),\n",
    "            ('scale', Normalizer())\n",
    "        ])),\n",
    "        ('rate_coupon_type', Pipeline([\n",
    "            ('extract', ColumnSelector(['User_coupon_type_used_rate', 'User_used_coupon_rate'])),\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan,  strategy='most_frequent')),\n",
    "            ('new', ExtractFeature()),\n",
    "            ('scale', Normalizer())\n",
    "        ])),\n",
    "        ('coupon_rate', Pipeline([\n",
    "            ('extract', ColumnSelector(['User_coupon_used_rate', 'Coupon_used_rate'])),\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan,  strategy='most_frequent')),\n",
    "            ('new', ExtractFeature()),\n",
    "            ('scale', Normalizer())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('skb', SelectKBest(chi2)),\n",
    "    ('sc4gbdt', StandardScaler()),\n",
    "    ('gbdt', GBDTTransformer()),\n",
    "    ('pca', PCA()),\n",
    "    ('clf', LogisticRegression(C=0.1, random_state=2, solver='lbfgs', class_weight='balanced', multi_class='multinomial', max_iter=5000, n_jobs=4))\n",
    "])\n",
    "\n",
    "pipe_lr.set_params(pca__n_components=1, skb__k=80).fit(model_train_df[fields+continous], model_train_df[label].values.ravel())\n",
    "\n",
    "# parameters = {\n",
    "# #     'gbdt__n_estimators': range(10, 70, 10),\n",
    "#     'pca__n_components': range(1, 9, 1),\n",
    "#     'skb__k': range(32, 129, 32)\n",
    "# }\n",
    "\n",
    "# cv = GridSearchCV(pipe_lr, parameters, scoring = 'roc_auc', n_jobs= 4)\n",
    "# cv.fit(model_train_df[fields+continous], model_train_df[label].values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def evaluate(result_df):\n",
    "    group = result_df.groupby(['Coupon_id'])\n",
    "    aucs = []\n",
    "    for i in group:\n",
    "        tmpdf = i[1]        \n",
    "        if len(tmpdf['Is_in_day_consume'].unique()) != 2:\n",
    "            continue\n",
    "            \n",
    "        fpr, tpr, thresholds = roc_curve(tmpdf['Is_in_day_consume'], tmpdf['Probability'], pos_label=1)\n",
    "        auc_score = auc(fpr,tpr)\n",
    "        aucs.append(auc_score)\n",
    "            \n",
    "    return np.average(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test_prob_y = pipe_lr.predict_proba(model_test_df[fields+continous])\n",
    "model_test_df['Probability'] = predict_test_prob_y[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6672552236176406"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>Merchant_id</th>\n",
       "      <th>Coupon_id</th>\n",
       "      <th>Discount_rate</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Date_received</th>\n",
       "      <th>Previous_date_received</th>\n",
       "      <th>Next_date_received</th>\n",
       "      <th>Previous_duration</th>\n",
       "      <th>Next_duration</th>\n",
       "      <th>...</th>\n",
       "      <th>Merchant_distance_receive_count</th>\n",
       "      <th>Merchant_distance_consume_count</th>\n",
       "      <th>Merchant_distance_used_count</th>\n",
       "      <th>Merchant_distance_receive_rate</th>\n",
       "      <th>Merchant_distance_used_rate</th>\n",
       "      <th>User_coupon_duration_used_mean</th>\n",
       "      <th>User_coupon_duration_used_max</th>\n",
       "      <th>User_coupon_duration_used_min</th>\n",
       "      <th>User_received_date_count</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92750</th>\n",
       "      <td>6013165</td>\n",
       "      <td>7963</td>\n",
       "      <td>5548</td>\n",
       "      <td>20:1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20160723</td>\n",
       "      <td>20160722.0</td>\n",
       "      <td>20160728.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92751</th>\n",
       "      <td>6013165</td>\n",
       "      <td>7963</td>\n",
       "      <td>5548</td>\n",
       "      <td>20:1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20160728</td>\n",
       "      <td>20160723.0</td>\n",
       "      <td>20160729.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92748</th>\n",
       "      <td>6013165</td>\n",
       "      <td>7963</td>\n",
       "      <td>5548</td>\n",
       "      <td>20:1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20160721</td>\n",
       "      <td>20160718.0</td>\n",
       "      <td>20160722.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92752</th>\n",
       "      <td>6013165</td>\n",
       "      <td>7963</td>\n",
       "      <td>5548</td>\n",
       "      <td>20:1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20160729</td>\n",
       "      <td>20160728.0</td>\n",
       "      <td>20160731.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92749</th>\n",
       "      <td>6013165</td>\n",
       "      <td>7963</td>\n",
       "      <td>5548</td>\n",
       "      <td>20:1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20160722</td>\n",
       "      <td>20160721.0</td>\n",
       "      <td>20160723.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       User_id  Merchant_id  Coupon_id Discount_rate  Distance  Date_received  \\\n",
       "92750  6013165         7963       5548          20:1       3.0       20160723   \n",
       "92751  6013165         7963       5548          20:1       3.0       20160728   \n",
       "92748  6013165         7963       5548          20:1       3.0       20160721   \n",
       "92752  6013165         7963       5548          20:1       3.0       20160729   \n",
       "92749  6013165         7963       5548          20:1       3.0       20160722   \n",
       "\n",
       "       Previous_date_received  Next_date_received  Previous_duration  \\\n",
       "92750              20160722.0          20160728.0                  2   \n",
       "92751              20160723.0          20160729.0                  6   \n",
       "92748              20160718.0          20160722.0                  4   \n",
       "92752              20160728.0          20160731.0                  2   \n",
       "92749              20160721.0          20160723.0                  2   \n",
       "\n",
       "       Next_duration     ...       Merchant_distance_receive_count  \\\n",
       "92750              6     ...                                   6.0   \n",
       "92751              2     ...                                   6.0   \n",
       "92748              2     ...                                   6.0   \n",
       "92752              3     ...                                   6.0   \n",
       "92749              2     ...                                   6.0   \n",
       "\n",
       "       Merchant_distance_consume_count  Merchant_distance_used_count  \\\n",
       "92750                             10.0                           3.0   \n",
       "92751                             10.0                           3.0   \n",
       "92748                             10.0                           3.0   \n",
       "92752                             10.0                           3.0   \n",
       "92749                             10.0                           3.0   \n",
       "\n",
       "       Merchant_distance_receive_rate  Merchant_distance_used_rate  \\\n",
       "92750                        0.315789                     0.157895   \n",
       "92751                        0.315789                     0.157895   \n",
       "92748                        0.315789                     0.157895   \n",
       "92752                        0.315789                     0.157895   \n",
       "92749                        0.315789                     0.157895   \n",
       "\n",
       "       User_coupon_duration_used_mean  User_coupon_duration_used_max  \\\n",
       "92750                        4.333333                            7.0   \n",
       "92751                        4.333333                            7.0   \n",
       "92748                        4.333333                            7.0   \n",
       "92752                        4.333333                            7.0   \n",
       "92749                        4.333333                            7.0   \n",
       "\n",
       "       User_coupon_duration_used_min  User_received_date_count  Probability  \n",
       "92750                            1.0                         7          1.0  \n",
       "92751                            1.0                         7          1.0  \n",
       "92748                            1.0                         7          1.0  \n",
       "92752                            1.0                         7          1.0  \n",
       "92749                            1.0                         7          1.0  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pred_df = pd.read_csv('lcm_test_features.csv')\n",
    "predict_prob_y = pipe_lr.predict_proba(model_pred_df[fields+continous])\n",
    "model_pred_df['Probability'] = predict_prob_y[:, 1]\n",
    "model_pred_df.sort_values(['Probability'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113640, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result_df = model_pred_df[['User_id', 'Coupon_id', 'Date_received', 'Probability']]\n",
    "final_result_df.to_csv('/Users/leewind/Desktop/submission_20190118.csv', index=False, header=False)\n",
    "final_result_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
