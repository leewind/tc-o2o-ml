{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import math\n",
    "import logging\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder,MinMaxScaler, Normalizer\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from mlxtend.preprocessing import DenseTransformer\n",
    "from mlxtend.feature_selection import ColumnSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('ai')\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(asctime)s  %(filename)s : %(levelname)s  %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "+ [Feature transformations with ensembles of trees](https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html)\n",
    "+ [sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "+ [机器学习之 sklearn中的pipeline](http://frankchen.xyz/2018/04/08/pipeline-in-machine-learning/)\n",
    "    - 使用pipeline做cross validation\n",
    "    - 自定义transformer\n",
    "    - FeatureUnion\n",
    "+ [Concatenating multiple feature extraction methods](https://scikit-learn.org/stable/auto_examples/compose/plot_feature_union.html#sphx-glr-auto-examples-compose-plot-feature-union-py)\n",
    "+ [sklearn.pipeline.Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "+ [gbdt+lr demo](https://github.com/princewen/tensorflow_practice/blob/master/recommendation/GBDT%2BLR-Demo/GBDT_LR.py)\n",
    "+ [推荐系统遇上深度学习(十)--GBDT+LR融合方案实战](https://zhuanlan.zhihu.com/p/37522339)\n",
    "+ [python︱sklearn一些小技巧的记录（训练集划分/pipelline/交叉验证等）](https://blog.csdn.net/sinat_26917383/article/details/77917881)\n",
    "+ [16.【进阶】特征提升之特征筛选----feature_selection](https://blog.csdn.net/jh1137921986/article/details/79822512)\n",
    "+ [使用sklearn优雅地进行数据挖掘](https://www.cnblogs.com/jasonfreak/p/5448462.html)\n",
    "+ [Kaggle机器学习之模型融合（stacking）心得](https://zhuanlan.zhihu.com/p/26890738)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "continous = [\n",
    "    'Discount',\n",
    "    'Previous_duration',\n",
    "    'Next_duration',\n",
    "    'Base_consume',\n",
    "    'User_receive_count',\n",
    "    'User_consume_count',\n",
    "    'User_used_count',\n",
    "    'User_not_used_count',\n",
    "    'User_used_coupon_rate',\n",
    "    'User_used_coupon_rate_max',\n",
    "    'User_used_coupon_rate_min',\n",
    "    'User_used_coupon_rate_mean',\n",
    "    'User_receive_coupon_merchant_count',\n",
    "    'User_consume_merchant_count',\n",
    "    'User_used_coupon_merchant_count',\n",
    "    'User_used_coupon_merchant_occ',\n",
    "    'User_receive_different_coupon_count',\n",
    "    'User_used_different_coupon_count',\n",
    "    'User_receive_different_coupon_occ',\n",
    "    'User_used_different_coupon_occ',\n",
    "    'User_receive_coupon_mean',\n",
    "    'User_used_coupon_mean',\n",
    "    'User_distance_used_mean',\n",
    "    'User_distance_used_max',\n",
    "    'User_distance_used_min',\n",
    "    'User_duration_used_mean',\n",
    "    'User_duration_used_max',\n",
    "    'User_duration_used_min',\n",
    "    'User_previous_duration_used_mean',\n",
    "    'User_previous_duration_used_max',\n",
    "    'User_previous_duration_used_min',\n",
    "    'User_next_duration_used_mean',\n",
    "    'User_next_duration_used_max',\n",
    "    'User_next_duration_used_min',\n",
    "    'Merchant_receive_count',\n",
    "    'Merchant_consume_count',\n",
    "    'Merchant_used_count',\n",
    "    'Merchant_not_used_count',\n",
    "    'Merchant_used_coupon_rate',\n",
    "    'Merchant_used_coupon_rate_max',\n",
    "    'Merchant_used_coupon_rate_min',\n",
    "    'Merchant_used_coupon_rate_mean',\n",
    "    'Merchant_receive_coupon_user_count',\n",
    "    'Merchant_consume_user_count',\n",
    "    'Merchant_used_coupon_user_count',\n",
    "    'Merchant_receive_coupon_user_occ',\n",
    "    'Merchant_consume_user_occ',\n",
    "    'Merchant_used_coupon_user_occ',\n",
    "    'Merchant_receive_different_coupon_count',\n",
    "    'Merchant_used_different_coupon_count',\n",
    "    'Merchant_receive_different_coupon_occ',\n",
    "    'Merchant_used_different_coupon_occ',\n",
    "    'Merchant_receive_coupon_mean',\n",
    "    'Merchant_used_coupon_mean',\n",
    "    'Merchant_receive_different_coupon_avg',\n",
    "    'Merchant_used_different_coupon_avg',\n",
    "    'Merchant_distance_used_mean',\n",
    "    'Merchant_distance_used_max',\n",
    "    'Merchant_distance_used_min',\n",
    "    'Merchant_duration_used_mean',\n",
    "    'Merchant_duration_used_max',\n",
    "    'Merchant_duration_used_min',\n",
    "    'Merchant_previous_duration_used_mean',\n",
    "    'Merchant_previous_duration_used_max',\n",
    "    'Merchant_previous_duration_used_min',\n",
    "    'Merchant_next_duration_used_mean',\n",
    "    'Merchant_next_duration_used_max',\n",
    "    'Merchant_next_duration_used_min',\n",
    "    'Coupon_received_count',\n",
    "    'Coupon_used_count',\n",
    "    'Coupon_used_rate',\n",
    "    'Coupon_duration_used_mean',\n",
    "    'Coupon_duration_used_max',\n",
    "    'Coupon_duration_used_min',\n",
    "    'Coupon_distance_used_mean',\n",
    "    'Coupon_distance_used_max',\n",
    "    'Coupon_distance_used_min',\n",
    "    'User_merchant_receive_count',\n",
    "    'User_merchant_consume_count',\n",
    "    'User_merchant_used_count',\n",
    "    'User_merchant_not_used_count',\n",
    "    'User_merchant_used_coupon_rate',\n",
    "    'User_merchant_not_used_coupon_rate',\n",
    "    'User_merchant_used_coupon_rate_4_merchant',\n",
    "    'User_merchant_not_used_coupon_rate_4_merchant',\n",
    "    'User_merchant_duration_used_mean',\n",
    "    'User_merchant_duration_used_max',\n",
    "    'User_merchant_duration_used_min',\n",
    "    'Online_user_receive_count',\n",
    "    'Online_user_consume_count',\n",
    "    'Online_user_used_count',\n",
    "    'Online_user_not_used_count',\n",
    "    'Online_user_used_coupon_rate',\n",
    "    'User_offline_consume_rate',\n",
    "    'User_offline_used_rate',\n",
    "    'User_offline_no_consume_coupon_rate',\n",
    "    'User_distance_receive_count',\n",
    "    'User_distance_consume_count',\n",
    "    'User_distance_used_count',\n",
    "    'User_distance_receive_rate',\n",
    "    'User_distance_consume_rate',\n",
    "    'User_distance_used_rate',\n",
    "    'User_coupon_type_receive_count',\n",
    "    'User_coupon_type_used_count',\n",
    "    'User_coupon_type_receive_rate',\n",
    "    'User_coupon_type_used_rate',\n",
    "    'User_coupon_receive_count',\n",
    "    'User_coupon_used_count',\n",
    "    'User_coupon_receive_rate',\n",
    "    'User_coupon_used_rate',\n",
    "    'Merchant_distance_receive_count',\n",
    "    'Merchant_distance_consume_count',\n",
    "    'Merchant_distance_used_count',\n",
    "    'Merchant_distance_receive_rate',\n",
    "    'Merchant_distance_used_rate',\n",
    "    'User_coupon_duration_used_mean',\n",
    "    'User_coupon_duration_used_max',\n",
    "    'User_coupon_duration_used_min',\n",
    "    'User_received_date_count'\n",
    "]\n",
    "\n",
    "features_prediction = [\n",
    "    'Duration'\n",
    "]\n",
    "\n",
    "\n",
    "fields = [\n",
    "    'Distance',\n",
    "    'Day_in_month_received',\n",
    "    'Day_in_week_received',\n",
    "    'Coupon_type'\n",
    "]\n",
    "\n",
    "label_duration = ['Duration']\n",
    "\n",
    "label = ['Is_in_day_consume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train_df = pd.read_csv('../features/lcm_train_features.csv')\n",
    "model_train_df = model_train_df[model_train_df['Coupon_id']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test_df = pd.read_csv('../features/lcm_train_test_features.csv')\n",
    "model_test_df = model_test_df[model_test_df['Coupon_id']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBDTTransformer(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.n_estimator = 256\n",
    "        self.model = GradientBoostingClassifier(max_depth=3, n_estimators=self.n_estimator, random_state=0)\n",
    "        \n",
    "    def fit(self, *args, **kwargs):\n",
    "        self.model.fit(*args, **kwargs)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        return self.model.apply(X)[:, :, 0]\n",
    "    \n",
    "class ExtractFeature(TransformerMixin):\n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        return pd.DataFrame(X[:,0] * X[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-22 14:05:19,933  <ipython-input-31-2be4a175f334> : INFO  Start training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('continuous', Pipeline(memory=None,\n",
       "     steps=[('extract', ColumnSelector(cols=['Discount', 'Previous_duration', 'Next_duration', 'Base_consume', 'User_receive_count', 'User_consume_count', 'User_used_count', 'User_not_used_cou...'l2',\n",
       "          random_state=2, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "          warm_start=False))])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('continuous', Pipeline([\n",
    "            ('extract', ColumnSelector(continous)),\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "            ('scale', Normalizer())\n",
    "        ])),\n",
    "        ('fields', Pipeline([\n",
    "            ('extract', ColumnSelector(fields)),\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan,  strategy='most_frequent')),\n",
    "            ('one_hot', OneHotEncoder(categories='auto')),\n",
    "            ('to_dense', DenseTransformer())\n",
    "        ])),\n",
    "        ('rate', Pipeline([\n",
    "            ('extract', ColumnSelector(['User_coupon_used_rate', 'User_used_coupon_rate'])),\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan,  strategy='most_frequent')),\n",
    "            ('new', ExtractFeature()),\n",
    "            ('scale', Normalizer())\n",
    "        ])),\n",
    "    ])),\n",
    "    ('skb', SelectKBest(chi2)),\n",
    "    ('sc4gbdt', StandardScaler()),\n",
    "    ('gbdt', GBDTTransformer()),\n",
    "    ('pca', PCA()),\n",
    "    ('clf', LogisticRegression(C=0.1, random_state=2, solver='lbfgs', class_weight='balanced', multi_class='multinomial', max_iter=5000, n_jobs=4))\n",
    "])\n",
    "\n",
    "logger.info('Start training')\n",
    "pipe_lr.set_params(\n",
    "    pca__n_components=2, \n",
    "    skb__k=64\n",
    ").fit(model_train_df[fields+continous], model_train_df[label].values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (746969,123) (10,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-57f30b59d25a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"darkgrid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe_lr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pca'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_train_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcontinous\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_train_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Is_in_day_consume'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/leewind-p6XO93Th/lib/python3.7/site-packages/sklearn/decomposition/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mX_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhiten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (746969,123) (10,) "
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "dataset = pipe_lr.named_steps['pca'].transform(model_train_df[fields+continous])\n",
    "dataset = pd.DataFrame(data=dataset, columns=['x', 'y'])\n",
    "dataset['label'] = model_train_df['Is_in_day_consume']\n",
    "\n",
    "sns.relplot(x=\"x\", y=\"y\", hue=\"label\", data=dataset);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ PCA降维之后的2维数据应该通过展示的方式来查看如果分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def evaluate(result_df):\n",
    "    group = result_df.groupby(['Coupon_id'])\n",
    "    aucs = []\n",
    "    for i in group:\n",
    "        tmpdf = i[1]        \n",
    "        if len(tmpdf['Is_in_day_consume'].unique()) != 2:\n",
    "            continue\n",
    "            \n",
    "        fpr, tpr, thresholds = roc_curve(tmpdf['Is_in_day_consume'], tmpdf['Probability'], pos_label=1)\n",
    "        auc_score = auc(fpr,tpr)\n",
    "        aucs.append(auc_score)\n",
    "            \n",
    "    return np.average(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test_prob_y = pipe_lr.predict_proba(model_test_df[fields+continous])\n",
    "model_test_df['Probability'] = predict_test_prob_y[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5282735702804426"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred_df = pd.read_csv('lcm_test_features.csv')\n",
    "predict_prob_y = pipe_lr.predict_proba(model_pred_df[fields+continous])\n",
    "model_pred_df['Probability'] = predict_prob_y[:, 1]\n",
    "model_pred_df.sort_values(['Probability'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_df = model_pred_df[['User_id', 'Coupon_id', 'Date_received', 'Probability']]\n",
    "final_result_df.to_csv('/Users/leewind/Desktop/submission_20190118.csv', index=False, header=False)\n",
    "final_result_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
