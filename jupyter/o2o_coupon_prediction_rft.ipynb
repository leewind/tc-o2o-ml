{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer, StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from mlxtend.preprocessing import DenseTransformer\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('ai')\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(asctime)s  %(filename)s : %(levelname)s  %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "+ [Feature transformations with ensembles of trees](https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html)\n",
    "+ [sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "+ [机器学习之 sklearn中的pipeline](http://frankchen.xyz/2018/04/08/pipeline-in-machine-learning/)\n",
    "    - 使用pipeline做cross validation\n",
    "    - 自定义transformer\n",
    "    - FeatureUnion\n",
    "+ [Concatenating multiple feature extraction methods](https://scikit-learn.org/stable/auto_examples/compose/plot_feature_union.html#sphx-glr-auto-examples-compose-plot-feature-union-py)\n",
    "+ [sklearn.pipeline.Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "+ [gbdt+lr demo](https://github.com/princewen/tensorflow_practice/blob/master/recommendation/GBDT%2BLR-Demo/GBDT_LR.py)\n",
    "+ [推荐系统遇上深度学习(十)--GBDT+LR融合方案实战](https://zhuanlan.zhihu.com/p/37522339)\n",
    "+ [python︱sklearn一些小技巧的记录（训练集划分/pipelline/交叉验证等）](https://blog.csdn.net/sinat_26917383/article/details/77917881)\n",
    "+ [16.【进阶】特征提升之特征筛选----feature_selection](https://blog.csdn.net/jh1137921986/article/details/79822512)\n",
    "+ [使用sklearn优雅地进行数据挖掘](https://www.cnblogs.com/jasonfreak/p/5448462.html)\n",
    "+ [Kaggle机器学习之模型融合（stacking）心得](https://zhuanlan.zhihu.com/p/26890738)\n",
    "+ [model_library_config](https://github.com/ChenglongChen/Kaggle_CrowdFlower/blob/master/Code/Model/model_library_config.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "continous = [\n",
    "    'Discount',\n",
    "    'Previous_duration',\n",
    "    'Next_duration',\n",
    "    'Base_consume',\n",
    "    'User_receive_count',\n",
    "    'User_consume_count',\n",
    "    'User_used_count',\n",
    "    'User_not_used_count',\n",
    "    'User_used_coupon_rate',\n",
    "    'User_used_coupon_rate_max',\n",
    "    'User_used_coupon_rate_min',\n",
    "    'User_used_coupon_rate_mean',\n",
    "    'User_receive_coupon_merchant_count',\n",
    "    'User_consume_merchant_count',\n",
    "    'User_used_coupon_merchant_count',\n",
    "    'User_used_coupon_merchant_occ',\n",
    "    'User_receive_different_coupon_count',\n",
    "    'User_used_different_coupon_count',\n",
    "    'User_receive_different_coupon_occ',\n",
    "    'User_used_different_coupon_occ',\n",
    "    'User_receive_coupon_mean',\n",
    "    'User_used_coupon_mean',\n",
    "    'User_distance_used_mean',\n",
    "    'User_distance_used_max',\n",
    "    'User_distance_used_min',\n",
    "    'User_duration_used_mean',\n",
    "    'User_duration_used_max',\n",
    "    'User_duration_used_min',\n",
    "    'User_previous_duration_used_mean',\n",
    "    'User_previous_duration_used_max',\n",
    "    'User_previous_duration_used_min',\n",
    "    'User_next_duration_used_mean',\n",
    "    'User_next_duration_used_max',\n",
    "    'User_next_duration_used_min',\n",
    "    'Merchant_receive_count',\n",
    "    'Merchant_consume_count',\n",
    "    'Merchant_used_count',\n",
    "    'Merchant_not_used_count',\n",
    "    'Merchant_used_coupon_rate',\n",
    "    'Merchant_used_coupon_rate_max',\n",
    "    'Merchant_used_coupon_rate_min',\n",
    "    'Merchant_used_coupon_rate_mean',\n",
    "    'Merchant_receive_coupon_user_count',\n",
    "    'Merchant_consume_user_count',\n",
    "    'Merchant_used_coupon_user_count',\n",
    "    'Merchant_receive_coupon_user_occ',\n",
    "    'Merchant_consume_user_occ',\n",
    "    'Merchant_used_coupon_user_occ',\n",
    "    'Merchant_receive_different_coupon_count',\n",
    "    'Merchant_used_different_coupon_count',\n",
    "    'Merchant_receive_different_coupon_occ',\n",
    "    'Merchant_used_different_coupon_occ',\n",
    "    'Merchant_receive_coupon_mean',\n",
    "    'Merchant_used_coupon_mean',\n",
    "    'Merchant_receive_different_coupon_avg',\n",
    "    'Merchant_used_different_coupon_avg',\n",
    "    'Merchant_distance_used_mean',\n",
    "    'Merchant_distance_used_max',\n",
    "    'Merchant_distance_used_min',\n",
    "    'Merchant_duration_used_mean',\n",
    "    'Merchant_duration_used_max',\n",
    "    'Merchant_duration_used_min',\n",
    "    'Merchant_previous_duration_used_mean',\n",
    "    'Merchant_previous_duration_used_max',\n",
    "    'Merchant_previous_duration_used_min',\n",
    "    'Merchant_next_duration_used_mean',\n",
    "    'Merchant_next_duration_used_max',\n",
    "    'Merchant_next_duration_used_min',\n",
    "    'Coupon_received_count',\n",
    "    'Coupon_used_count',\n",
    "    'Coupon_used_rate',\n",
    "    'Coupon_duration_used_mean',\n",
    "    'Coupon_duration_used_max',\n",
    "    'Coupon_duration_used_min',\n",
    "    'Coupon_distance_used_mean',\n",
    "    'Coupon_distance_used_max',\n",
    "    'Coupon_distance_used_min',\n",
    "    'User_merchant_receive_count',\n",
    "    'User_merchant_consume_count',\n",
    "    'User_merchant_used_count',\n",
    "    'User_merchant_not_used_count',\n",
    "    'User_merchant_used_coupon_rate',\n",
    "    'User_merchant_not_used_coupon_rate',\n",
    "    'User_merchant_used_coupon_rate_4_merchant',\n",
    "    'User_merchant_not_used_coupon_rate_4_merchant',\n",
    "    'User_merchant_duration_used_mean',\n",
    "    'User_merchant_duration_used_max',\n",
    "    'User_merchant_duration_used_min',\n",
    "    'Online_user_receive_count',\n",
    "    'Online_user_consume_count',\n",
    "    'Online_user_used_count',\n",
    "    'Online_user_not_used_count',\n",
    "    'Online_user_used_coupon_rate',\n",
    "    'User_offline_consume_rate',\n",
    "    'User_offline_used_rate',\n",
    "    'User_offline_no_consume_coupon_rate',\n",
    "    'User_distance_receive_count',\n",
    "    'User_distance_consume_count',\n",
    "    'User_distance_used_count',\n",
    "    'User_distance_receive_rate',\n",
    "    'User_distance_consume_rate',\n",
    "    'User_distance_used_rate',\n",
    "    'User_coupon_type_receive_count',\n",
    "    'User_coupon_type_used_count',\n",
    "    'User_coupon_type_receive_rate',\n",
    "    'User_coupon_type_used_rate',\n",
    "    'User_coupon_receive_count',\n",
    "    'User_coupon_used_count',\n",
    "    'User_coupon_receive_rate',\n",
    "    'User_coupon_used_rate',\n",
    "    'Merchant_distance_receive_count',\n",
    "    'Merchant_distance_consume_count',\n",
    "    'Merchant_distance_used_count',\n",
    "    'Merchant_distance_receive_rate',\n",
    "    'Merchant_distance_used_rate',\n",
    "    'User_coupon_duration_used_mean',\n",
    "    'User_coupon_duration_used_max',\n",
    "    'User_coupon_duration_used_min',\n",
    "    'User_received_date_count'\n",
    "]\n",
    "\n",
    "\n",
    "fields = [\n",
    "    'Distance',\n",
    "    'Day_in_month_received',\n",
    "    'Day_in_week_received',\n",
    "    'Coupon_type'\n",
    "]\n",
    "\n",
    "label = ['Is_in_day_consume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train_df = pd.read_csv('../features/lcm_train_features.csv')\n",
    "model_train_df = model_train_df[model_train_df['Coupon_id']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test_df = pd.read_csv('../features/lcm_train_test_features.csv')\n",
    "model_test_df = model_test_df[model_test_df['Coupon_id']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractFeature(TransformerMixin):\n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        return pd.DataFrame(X[:,0] * X[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('continuous', Pipeline(memory=None,\n",
       "     steps=[('extract', ColumnSelector(cols=['Discount', 'Previous_duration', 'Next_duration', 'Base_consume', 'User_receive_count', 'User_consume_count', 'User_used_count', 'User_not_used_cou...  transformer_weights=None)), ('sc4gbdt', StandardScaler(copy=True, with_mean=True, with_std=True))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('continuous', Pipeline([\n",
    "            ('extract', ColumnSelector(continous + fields)),\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "            ('scale', Normalizer())\n",
    "        ])),\n",
    "#         ('fields', Pipeline([\n",
    "#             ('extract', ColumnSelector(fields)),\n",
    "#             ('imputer', SimpleImputer(missing_values=np.nan,  strategy='most_frequent')),\n",
    "#             ('one_hot', OneHotEncoder(categories='auto')),\n",
    "#             ('to_dense', DenseTransformer())\n",
    "#         ])),\n",
    "        ('rate', Pipeline([\n",
    "            ('extract', ColumnSelector(['User_coupon_used_rate', 'User_used_coupon_rate'])),\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan,  strategy='most_frequent')),\n",
    "            ('new', ExtractFeature()),\n",
    "            ('scale', Normalizer())\n",
    "        ]))\n",
    "    ])),\n",
    "#     ('skb', SelectKBest(chi2, k=80)),\n",
    "    ('sc4gbdt', StandardScaler())\n",
    "])\n",
    "\n",
    "fp.fit(model_train_df[fields+continous], model_train_df[label].values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_x = fp.transform(model_train_df[fields+continous])\n",
    "train_dataset_y = model_train_df[label].values.ravel()\n",
    "\n",
    "valid_dataset_x = fp.transform(model_test_df[fields+continous])\n",
    "valid_dataset_y = model_test_df[label].values.ravel()\n",
    "\n",
    "xgbtrain = xgb.DMatrix(train_dataset_x, label=train_dataset_y)\n",
    "xgbvalid = xgb.DMatrix(valid_dataset_x, label=valid_dataset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(result_df):\n",
    "    group = result_df.groupby(['Coupon_id'])\n",
    "    aucs = []\n",
    "    for i in group:\n",
    "        tmpdf = i[1]        \n",
    "        if len(tmpdf['Is_in_day_consume'].unique()) != 2:\n",
    "            continue\n",
    "            \n",
    "        fpr, tpr, thresholds = roc_curve(tmpdf['Is_in_day_consume'], tmpdf['Probability'], pos_label=1)\n",
    "        auc_score = auc(fpr,tpr)\n",
    "        aucs.append(auc_score)\n",
    "            \n",
    "    return np.average(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:09:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 98 extra nodes, 4 pruned nodes, max_depth=12\n",
      "[125]\ttrain-auc:0.999822\tvalidate-auc:0.564556\n",
      "[10:10:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 204 extra nodes, 12 pruned nodes, max_depth=12\n",
      "[126]\ttrain-auc:0.999823\tvalidate-auc:0.564556\n",
      "[10:10:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 118 extra nodes, 2 pruned nodes, max_depth=12\n",
      "[127]\ttrain-auc:0.999823\tvalidate-auc:0.564556\n",
      "[10:10:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 104 extra nodes, 2 pruned nodes, max_depth=12\n",
      "[128]\ttrain-auc:0.999824\tvalidate-auc:0.564556\n",
      "[10:10:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 136 extra nodes, 4 pruned nodes, max_depth=12\n",
      "[129]\ttrain-auc:0.999824\tvalidate-auc:0.564556\n",
      "[10:10:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 102 extra nodes, 6 pruned nodes, max_depth=12\n",
      "[130]\ttrain-auc:0.999824\tvalidate-auc:0.564556\n",
      "[10:10:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 168 extra nodes, 8 pruned nodes, max_depth=12\n",
      "[131]\ttrain-auc:0.999825\tvalidate-auc:0.564556\n",
      "[10:11:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 140 extra nodes, 2 pruned nodes, max_depth=12\n",
      "[132]\ttrain-auc:0.999826\tvalidate-auc:0.564556\n",
      "[10:11:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 156 extra nodes, 4 pruned nodes, max_depth=12\n",
      "[133]\ttrain-auc:0.999826\tvalidate-auc:0.564556\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    # gbtree and dart use tree based models while gblinear uses linear functions.\n",
    "    'booster': 'gbtree',\n",
    "    #  Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized\n",
    "    'objective': 'rank:pairwise',\n",
    "    # auc: Area under the curve\n",
    "    'eval_metric': 'auc',\n",
    "    # Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n",
    "    'gamma': 0.1,\n",
    "    'min_child_weight': 1.1,\n",
    "    \n",
    "    'max_depth':4,\n",
    "#     'max_depth': 12,\n",
    "    # Maximum number of nodes to be added\n",
    "    'max_leaves': 128,\n",
    "    # L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "    'lambda': 3,\n",
    "    \n",
    "    'alpha': 2,\n",
    "    # Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \n",
    "    'subsample': 0.7,\n",
    "    # This is a family of parameters for subsampling of columns.\n",
    "    'colsample_bytree': 0.7,\n",
    "    'colsample_bylevel': 0.7,\n",
    "    # learning_rate\n",
    "    'eta': 0.01,\n",
    "    # Exact greedy algorithm\n",
    "    'tree_method': 'exact',\n",
    "    # Random number seed.\n",
    "    'seed': 0,\n",
    "    'nthread': 4,\n",
    "    # Verbosity of printing messages. Valid values are 0 (silent),\n",
    "    'verbosity': 0,\n",
    "    'metric_freq': 100,\n",
    "}\n",
    "\n",
    "watchlist = [(xgbtrain, 'train'), (xgbvalid, 'validate')]\n",
    "\n",
    "logging.info('train begin')\n",
    "model = xgb.train(params, xgbtrain, num_boost_round=200, evals=watchlist)\n",
    "logging.info('train end')\n",
    "model.save_model('../model/xgb.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_min_num_round = 10\n",
    "xgb_max_num_round = 500\n",
    "xgb_num_round_step = 10\n",
    "\n",
    "xgb_random_seed = 2018\n",
    "xgb_nthread = 2\n",
    "xgb_dmatrix_silent = True\n",
    "\n",
    "space = {\n",
    "    'booster': 'gblinear',\n",
    "    'objective': 'binary:logistic',\n",
    "    \n",
    "    'max_depth': hp.quniform('max_depth', 1, 10, 1),\n",
    "    'eta' : hp.quniform('eta', 0.01, 1, 0.01),\n",
    "    'lambda' : hp.quniform('lambda', 0, 5, 0.05),\n",
    "    'alpha' : hp.quniform('alpha', 0, 0.5, 0.005),\n",
    "    'lambda_bias' : hp.quniform('lambda_bias', 0, 3, 0.1),\n",
    "    'num_round' : hp.quniform('num_round', xgb_min_num_round, xgb_max_num_round, xgb_num_round_step),\n",
    "    \n",
    "    'nthread': xgb_nthread,\n",
    "    'silent' : 1,\n",
    "    'seed': xgb_random_seed,\n",
    "    \"max_evals\": 200,\n",
    "    'eval_metric': 'auc'\n",
    "}\n",
    "\n",
    "numOfClass = 2\n",
    "\n",
    "watchlist  = [(xgbtrain, 'train')]\n",
    "\n",
    "def objective(params):\n",
    "    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Tuning\"\"\"\n",
    "    \n",
    "    logger.info(params)    \n",
    "    params['num_round'] = int(params['num_round'])\n",
    "    \n",
    "#     bst = xgb.train(params, xgbtrain, params['num_round'], watchlist)\n",
    "\n",
    "    bst = xgb.train(params, xgbtrain, params['num_round'])\n",
    "    pred = bst.predict(xgbvalid)\n",
    "    \n",
    "    logger.info(pred)\n",
    "    \n",
    "    model_test_df['Probability'] = pred\n",
    "    score = evaluate(model_test_df)\n",
    "    logging.info('Socre is %f' % score)\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    loss = 1 - score\n",
    "    \n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "\n",
    "def evaluate(result_df):\n",
    "    group = result_df.groupby(['Coupon_id'])\n",
    "    aucs = []\n",
    "    for i in group:\n",
    "        tmpdf = i[1]        \n",
    "        if len(tmpdf['Is_in_day_consume'].unique()) != 2:\n",
    "            continue\n",
    "            \n",
    "        fpr, tpr, thresholds = roc_curve(tmpdf['Is_in_day_consume'], tmpdf['Probability'], pos_label=1)\n",
    "        auc_score = auc(fpr,tpr)\n",
    "        aucs.append(auc_score)\n",
    "            \n",
    "    return np.average(aucs)\n",
    "\n",
    "MAX_EVALS = 200\n",
    "\n",
    "# Optimize\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = Trials())\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDT 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skl_min_n_estimators = 10\n",
    "skl_max_n_estimators = 500\n",
    "skl_n_estimators_step = 10\n",
    "skl_n_jobs = 2\n",
    "skl_random_seed = 2018\n",
    "\n",
    "## random forest tree classifier\n",
    "space = {\n",
    "    'n_estimators': hp.quniform(\"n_estimators\", skl_min_n_estimators, skl_max_n_estimators, skl_n_estimators_step),\n",
    "    'learning_rate': hp.quniform(\"learning_rate\", 0.01, 0.5, 0.01),\n",
    "    'max_features': hp.quniform(\"max_features\", 0.05, 1.0, 0.05),\n",
    "    'max_depth': hp.quniform('max_depth', 1, 15, 1),\n",
    "    'subsample': hp.quniform('subsample', 0.5, 1, 0.1),\n",
    "    'random_state': skl_random_seed\n",
    "}\n",
    "\n",
    "def objective(params):\n",
    "    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Tuning\"\"\"\n",
    "    \n",
    "    logger.info(params)\n",
    "    \n",
    "    gbcf = GradientBoostingClassifier(\n",
    "        n_estimators=int(params['n_estimators']), \n",
    "        max_features=params['max_features'], \n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_depth=params['max_depth'], \n",
    "        subsample=params['subsample'],\n",
    "        random_state=params['random_state']\n",
    "    )\n",
    "    \n",
    "    gbcf.fit(train_dataset_x, train_dataset_y)\n",
    "    \n",
    "    predict_test_prob_y = gbcf.predict_proba(valid_dataset_x)\n",
    "    model_test_df['Probability'] = predict_test_prob_y[:, 1]\n",
    "    \n",
    "    score = evaluate(model_test_df)\n",
    "\n",
    "#     gbdt = GradientBoostingRegressor(\n",
    "#         n_estimators=int(params['n_estimators']), \n",
    "#         max_features=params['max_features'], \n",
    "#         learning_rate=params['learning_rate'],\n",
    "#         max_depth=params['max_depth'], \n",
    "#         subsample=params['subsample'],\n",
    "#         random_state=params['random_state'],\n",
    "#         verbose=1\n",
    "#     )\n",
    "    \n",
    "#     gbdt.fit(train_dataset_x, train_dataset_y)\n",
    "#     score = gbdt.score(valid_dataset_x, valid_dataset_y)\n",
    "    logging.info('Socre is %f' % score)\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    loss = 1 - score\n",
    "    \n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "\n",
    "def evaluate(result_df):\n",
    "    group = result_df.groupby(['Coupon_id'])\n",
    "    aucs = []\n",
    "    for i in group:\n",
    "        tmpdf = i[1]        \n",
    "        if len(tmpdf['Is_in_day_consume'].unique()) != 2:\n",
    "            continue\n",
    "            \n",
    "        fpr, tpr, thresholds = roc_curve(tmpdf['Is_in_day_consume'], tmpdf['Probability'], pos_label=1)\n",
    "        auc_score = auc(fpr,tpr)\n",
    "        aucs.append(auc_score)\n",
    "            \n",
    "    return np.average(aucs)\n",
    "\n",
    "MAX_EVALS = 500\n",
    "\n",
    "# Optimize\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = Trials())\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>Merchant_id</th>\n",
       "      <th>Coupon_id</th>\n",
       "      <th>Discount_rate</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Date_received</th>\n",
       "      <th>Previous_date_received</th>\n",
       "      <th>Next_date_received</th>\n",
       "      <th>Previous_duration</th>\n",
       "      <th>Next_duration</th>\n",
       "      <th>...</th>\n",
       "      <th>Merchant_distance_receive_count</th>\n",
       "      <th>Merchant_distance_consume_count</th>\n",
       "      <th>Merchant_distance_used_count</th>\n",
       "      <th>Merchant_distance_receive_rate</th>\n",
       "      <th>Merchant_distance_used_rate</th>\n",
       "      <th>User_coupon_duration_used_mean</th>\n",
       "      <th>User_coupon_duration_used_max</th>\n",
       "      <th>User_coupon_duration_used_min</th>\n",
       "      <th>User_received_date_count</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92750</th>\n",
       "      <td>6013165</td>\n",
       "      <td>7963</td>\n",
       "      <td>5548</td>\n",
       "      <td>20:1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20160723</td>\n",
       "      <td>20160722.0</td>\n",
       "      <td>20160728.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92751</th>\n",
       "      <td>6013165</td>\n",
       "      <td>7963</td>\n",
       "      <td>5548</td>\n",
       "      <td>20:1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20160728</td>\n",
       "      <td>20160723.0</td>\n",
       "      <td>20160729.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92748</th>\n",
       "      <td>6013165</td>\n",
       "      <td>7963</td>\n",
       "      <td>5548</td>\n",
       "      <td>20:1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20160721</td>\n",
       "      <td>20160718.0</td>\n",
       "      <td>20160722.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92752</th>\n",
       "      <td>6013165</td>\n",
       "      <td>7963</td>\n",
       "      <td>5548</td>\n",
       "      <td>20:1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20160729</td>\n",
       "      <td>20160728.0</td>\n",
       "      <td>20160731.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92749</th>\n",
       "      <td>6013165</td>\n",
       "      <td>7963</td>\n",
       "      <td>5548</td>\n",
       "      <td>20:1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20160722</td>\n",
       "      <td>20160721.0</td>\n",
       "      <td>20160723.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       User_id  Merchant_id  Coupon_id Discount_rate  Distance  Date_received  \\\n",
       "92750  6013165         7963       5548          20:1       3.0       20160723   \n",
       "92751  6013165         7963       5548          20:1       3.0       20160728   \n",
       "92748  6013165         7963       5548          20:1       3.0       20160721   \n",
       "92752  6013165         7963       5548          20:1       3.0       20160729   \n",
       "92749  6013165         7963       5548          20:1       3.0       20160722   \n",
       "\n",
       "       Previous_date_received  Next_date_received  Previous_duration  \\\n",
       "92750              20160722.0          20160728.0                  2   \n",
       "92751              20160723.0          20160729.0                  6   \n",
       "92748              20160718.0          20160722.0                  4   \n",
       "92752              20160728.0          20160731.0                  2   \n",
       "92749              20160721.0          20160723.0                  2   \n",
       "\n",
       "       Next_duration     ...       Merchant_distance_receive_count  \\\n",
       "92750              6     ...                                   6.0   \n",
       "92751              2     ...                                   6.0   \n",
       "92748              2     ...                                   6.0   \n",
       "92752              3     ...                                   6.0   \n",
       "92749              2     ...                                   6.0   \n",
       "\n",
       "       Merchant_distance_consume_count  Merchant_distance_used_count  \\\n",
       "92750                             10.0                           3.0   \n",
       "92751                             10.0                           3.0   \n",
       "92748                             10.0                           3.0   \n",
       "92752                             10.0                           3.0   \n",
       "92749                             10.0                           3.0   \n",
       "\n",
       "       Merchant_distance_receive_rate  Merchant_distance_used_rate  \\\n",
       "92750                        0.315789                     0.157895   \n",
       "92751                        0.315789                     0.157895   \n",
       "92748                        0.315789                     0.157895   \n",
       "92752                        0.315789                     0.157895   \n",
       "92749                        0.315789                     0.157895   \n",
       "\n",
       "       User_coupon_duration_used_mean  User_coupon_duration_used_max  \\\n",
       "92750                        4.333333                            7.0   \n",
       "92751                        4.333333                            7.0   \n",
       "92748                        4.333333                            7.0   \n",
       "92752                        4.333333                            7.0   \n",
       "92749                        4.333333                            7.0   \n",
       "\n",
       "       User_coupon_duration_used_min  User_received_date_count  Probability  \n",
       "92750                            1.0                         7          1.0  \n",
       "92751                            1.0                         7          1.0  \n",
       "92748                            1.0                         7          1.0  \n",
       "92752                            1.0                         7          1.0  \n",
       "92749                            1.0                         7          1.0  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pred_df = pd.read_csv('lcm_test_features.csv')\n",
    "predict_prob_y = pipe_lr.predict_proba(model_pred_df[fields+continous])\n",
    "model_pred_df['Probability'] = predict_prob_y[:, 1]\n",
    "model_pred_df.sort_values(['Probability'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113640, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result_df = model_pred_df[['User_id', 'Coupon_id', 'Date_received', 'Probability']]\n",
    "final_result_df.to_csv('/Users/leewind/Desktop/submission_20190118.csv', index=False, header=False)\n",
    "final_result_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
