{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer, StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, chi2, SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from mlxtend.preprocessing import DenseTransformer\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('ai')\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(asctime)s  %(filename)s : %(levelname)s  %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "+ [Feature transformations with ensembles of trees](https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html)\n",
    "+ [sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "+ [机器学习之 sklearn中的pipeline](http://frankchen.xyz/2018/04/08/pipeline-in-machine-learning/)\n",
    "    - 使用pipeline做cross validation\n",
    "    - 自定义transformer\n",
    "    - FeatureUnion\n",
    "+ [Concatenating multiple feature extraction methods](https://scikit-learn.org/stable/auto_examples/compose/plot_feature_union.html#sphx-glr-auto-examples-compose-plot-feature-union-py)\n",
    "+ [sklearn.pipeline.Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "+ [gbdt+lr demo](https://github.com/princewen/tensorflow_practice/blob/master/recommendation/GBDT%2BLR-Demo/GBDT_LR.py)\n",
    "+ [推荐系统遇上深度学习(十)--GBDT+LR融合方案实战](https://zhuanlan.zhihu.com/p/37522339)\n",
    "+ [python︱sklearn一些小技巧的记录（训练集划分/pipelline/交叉验证等）](https://blog.csdn.net/sinat_26917383/article/details/77917881)\n",
    "+ [16.【进阶】特征提升之特征筛选----feature_selection](https://blog.csdn.net/jh1137921986/article/details/79822512)\n",
    "+ [使用sklearn优雅地进行数据挖掘](https://www.cnblogs.com/jasonfreak/p/5448462.html)\n",
    "+ [Kaggle机器学习之模型融合（stacking）心得](https://zhuanlan.zhihu.com/p/26890738)\n",
    "+ [model_library_config](https://github.com/ChenglongChen/Kaggle_CrowdFlower/blob/master/Code/Model/model_library_config.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../features/lcm_base_features.csv')\n",
    "user_features_df = pd.read_csv('../features/lcm_user_features.csv')\n",
    "merchant_features_df = pd.read_csv('../features/lcm_merchant_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipipe = Pipeline([\n",
    "    ('pca', PCA(n_components=2)),\n",
    "    ('scale', MinMaxScaler()),\n",
    "])\n",
    "\n",
    "def get_factor(df, key, prefix):\n",
    "    id_df = df[[key]]\n",
    "    output_df = df.drop([key], axis=1)\n",
    "\n",
    "    ipipe.fit(output_df)\n",
    "    factors = ipipe.transform(output_df)\n",
    "    factors_df = pd.DataFrame(data=factors, columns=[prefix + '_factor_alpha', prefix + '_factor_beta'])\n",
    "    factors_df[key] = id_df[key]\n",
    "    return factors_df\n",
    "\n",
    "df = pd.merge(df, get_factor(user_features_df, 'User_id', 'User'), on=['User_id'], how='left')\n",
    "df = pd.merge(df, get_factor(merchant_features_df, 'Merchant_id', 'Merchant'), on=['Merchant_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['User_id', 'Merchant_id', 'Coupon_id', 'Distance', 'Date_received',\n",
       "       'Is_in_day_consume', 'Discount', 'Base_consume', 'Discount_money',\n",
       "       'Day_in_month', 'Day_in_week', 'Coupon_type', 'Offline_consume',\n",
       "       'Duration', 'User_factor_alpha', 'User_factor_beta',\n",
       "       'Merchant_factor_alpha', 'Merchant_factor_beta'], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "continous = [\n",
    "    'Discount', \n",
    "    'Base_consume', \n",
    "    'Discount_money',\n",
    "    'User_factor_alpha',\n",
    "    'User_factor_beta',\n",
    "    'Merchant_factor_alpha',\n",
    "    'Merchant_factor_beta'\n",
    "]\n",
    "\n",
    "fields = [\n",
    "    'Distance',\n",
    "    'Day_in_month',\n",
    "    'Day_in_week',\n",
    "    'Coupon_type'\n",
    "]\n",
    "\n",
    "label = ['Is_in_day_consume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_train_df = pd.read_csv('../features/lcm_base_features.csv')\n",
    "# model_train_df = model_train_df[model_train_df['Coupon_id']>0]\n",
    "model_train_df = df[df['Date_received']<20160501]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_test_df = pd.read_csv('../features/lcm_train_test_features.csv')\n",
    "# model_test_df = model_test_df[model_test_df['Coupon_id']>0]\n",
    "model_test_df = model_train_df = df[df['Date_received']>=20160501]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractFeature(TransformerMixin):\n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        return pd.DataFrame(X[:,0] * X[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('continuous', Pipeline(memory=None,\n",
       "     steps=[('extract', ColumnSelector(cols=['Discount', 'Base_consume', 'Discount_money', 'User_factor_alpha', 'User_factor_beta', 'Merchant_factor_alpha', 'Merchant_factor_beta'],\n",
       "        drop_axis=False)), ('imputer', SimpleImputer(copy=True,...n='error',\n",
       "       n_values=None, sparse=True)), ('to_dense', DenseTransformer(return_copy=True))]))],\n",
       "       transformer_weights=None))])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('continuous', Pipeline([\n",
    "            ('extract', ColumnSelector(continous)),\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "            ('scale', Normalizer())\n",
    "        ])),\n",
    "        ('fields', Pipeline([\n",
    "            ('extract', ColumnSelector(fields)),\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan,  strategy='most_frequent')),\n",
    "            ('one_hot', OneHotEncoder(categories='auto')),\n",
    "            ('to_dense', DenseTransformer())\n",
    "        ])),\n",
    "    ])),\n",
    "#     ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False))),\n",
    "#     ('skb', SelectKBest(chi2, k=64)),\n",
    "#     ('sc4gbdt', StandardScaler())\n",
    "])\n",
    "\n",
    "fp.fit(model_train_df[fields+continous], model_train_df[label].values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_x = fp.transform(model_train_df[fields+continous])\n",
    "train_dataset_y = model_train_df[label].values.ravel()\n",
    "\n",
    "valid_dataset_x = fp.transform(model_test_df[fields+continous])\n",
    "valid_dataset_y = model_test_df[label].values.ravel()\n",
    "\n",
    "xgbtrain = xgb.DMatrix(train_dataset_x, label=train_dataset_y)\n",
    "xgbvalid = xgb.DMatrix(valid_dataset_x, label=valid_dataset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(result_df):\n",
    "    group = result_df.groupby(['Coupon_id'])\n",
    "    aucs = []\n",
    "    for i in group:\n",
    "        tmpdf = i[1]        \n",
    "        if len(tmpdf['Is_in_day_consume'].unique()) != 2:\n",
    "            continue\n",
    "            \n",
    "        fpr, tpr, thresholds = roc_curve(tmpdf['Is_in_day_consume'], tmpdf['Probability'], pos_label=1)\n",
    "        auc_score = auc(fpr,tpr)\n",
    "        aucs.append(auc_score)\n",
    "            \n",
    "    return np.average(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_min_num_round = 10\n",
    "xgb_max_num_round = 500\n",
    "xgb_num_round_step = 10\n",
    "\n",
    "xgb_random_seed = 2018\n",
    "xgb_nthread = 4\n",
    "xgb_dmatrix_silent = True\n",
    "\n",
    "space = {\n",
    "    'booster': 'gblinear',\n",
    "    'objective': 'rank:pairwise',\n",
    "    'nthread': xgb_nthread,\n",
    "    'silent' : True,\n",
    "    'seed': xgb_random_seed,\n",
    "    \"max_evals\": 200,\n",
    "    'eval_metric': 'auc',\n",
    "    'max_depth': hp.quniform('max_depth', 6, 18, 1),\n",
    "    'eta' : hp.quniform('eta', 0.01, 1, 0.01),\n",
    "#     'lambda' : hp.quniform('lambda', 0, 5, 0.05),\n",
    "#     'alpha' : hp.quniform('alpha', 0, 0.5, 0.005),\n",
    "#     'lambda_bias' : hp.quniform('lambda_bias', 0, 3, 0.1),\n",
    "#     'num_round' : hp.quniform('num_round', xgb_min_num_round, xgb_max_num_round, xgb_num_round_step),\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 500, 50),\n",
    "}\n",
    "\n",
    "watchlist = [(train_dataset_x, train_dataset_y), (valid_dataset_x, valid_dataset_y)]\n",
    "\n",
    "def objective(params):\n",
    "    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Tuning\"\"\"\n",
    "    \n",
    "    logger.info(params)    \n",
    "    bst = xgb.sklearn.XGBClassifier(\n",
    "        nthread=params['nthread'],\n",
    "        learn_rate=params['eta'],\n",
    "        max_depth=int(params['max_depth']),\n",
    "        min_child_weight=1.1,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.7,\n",
    "        colsample_bylevel=0.7,\n",
    "        objective=params['objective'],\n",
    "        n_estimators=int(params['n_estimators']),\n",
    "        gamma=0.1,\n",
    "        reg_alpha=0,\n",
    "        reg_lambda=1,\n",
    "        max_delta_step=0,\n",
    "        scale_pos_weight=1,\n",
    "        silent=params['silent']\n",
    "    )\n",
    "    bst.fit(train_dataset_x, train_dataset_y, eval_set=watchlist, eval_metric=params['eval_metric'], early_stopping_rounds=10)\n",
    "    \n",
    "    predict_test_prob_y = bst.predict_proba(valid_dataset_x)\n",
    "    model_test_df['Probability'] = predict_test_prob_y[:, 1]\n",
    "    score = evaluate(model_test_df)\n",
    "    logging.info('Socre is %f' % score)\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    loss = 1 - score\n",
    "    \n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "\n",
    "MAX_EVALS = 200\n",
    "\n",
    "# Optimize\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = Trials())\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:54:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2272 extra nodes, 290 pruned nodes, max_depth=18\n",
      "[414]\tvalidation_0-auc:0.974762\tvalidation_1-auc:0.974762\n",
      "[22:54:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3488 extra nodes, 382 pruned nodes, max_depth=18\n",
      "[415]\tvalidation_0-auc:0.974804\tvalidation_1-auc:0.974804\n",
      "[22:54:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2840 extra nodes, 352 pruned nodes, max_depth=18\n",
      "[416]\tvalidation_0-auc:0.974825\tvalidation_1-auc:0.974825\n",
      "[22:54:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3500 extra nodes, 470 pruned nodes, max_depth=18\n",
      "[417]\tvalidation_0-auc:0.974864\tvalidation_1-auc:0.974864\n",
      "[22:55:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2768 extra nodes, 260 pruned nodes, max_depth=18\n",
      "[418]\tvalidation_0-auc:0.974886\tvalidation_1-auc:0.974886\n",
      "[22:55:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3000 extra nodes, 312 pruned nodes, max_depth=18\n",
      "[419]\tvalidation_0-auc:0.974921\tvalidation_1-auc:0.974921\n",
      "[22:55:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4858 extra nodes, 608 pruned nodes, max_depth=18\n",
      "[420]\tvalidation_0-auc:0.97497\tvalidation_1-auc:0.97497\n",
      "[22:55:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3454 extra nodes, 450 pruned nodes, max_depth=18\n",
      "[421]\tvalidation_0-auc:0.975009\tvalidation_1-auc:0.975009\n",
      "[22:55:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3210 extra nodes, 426 pruned nodes, max_depth=18\n",
      "[422]\tvalidation_0-auc:0.975044\tvalidation_1-auc:0.975044\n",
      "[22:55:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2466 extra nodes, 364 pruned nodes, max_depth=18\n",
      "[423]\tvalidation_0-auc:0.975072\tvalidation_1-auc:0.975072\n",
      "[22:55:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2136 extra nodes, 292 pruned nodes, max_depth=18\n",
      "[424]\tvalidation_0-auc:0.975082\tvalidation_1-auc:0.975082\n",
      "[22:55:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2938 extra nodes, 360 pruned nodes, max_depth=18\n",
      "[425]\tvalidation_0-auc:0.975116\tvalidation_1-auc:0.975116\n",
      "[22:55:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3458 extra nodes, 408 pruned nodes, max_depth=18\n",
      "[426]\tvalidation_0-auc:0.975159\tvalidation_1-auc:0.975159\n",
      "[22:55:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2106 extra nodes, 296 pruned nodes, max_depth=18\n",
      "[427]\tvalidation_0-auc:0.975183\tvalidation_1-auc:0.975183\n",
      "[22:55:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3534 extra nodes, 454 pruned nodes, max_depth=18\n",
      "[428]\tvalidation_0-auc:0.97521\tvalidation_1-auc:0.97521\n",
      "[22:55:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2790 extra nodes, 348 pruned nodes, max_depth=18\n",
      "[429]\tvalidation_0-auc:0.975228\tvalidation_1-auc:0.975228\n",
      "[22:55:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3304 extra nodes, 392 pruned nodes, max_depth=18\n",
      "[430]\tvalidation_0-auc:0.975263\tvalidation_1-auc:0.975263\n",
      "[22:55:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2464 extra nodes, 344 pruned nodes, max_depth=18\n",
      "[431]\tvalidation_0-auc:0.975273\tvalidation_1-auc:0.975273\n",
      "[22:55:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3766 extra nodes, 506 pruned nodes, max_depth=18\n",
      "[432]\tvalidation_0-auc:0.975301\tvalidation_1-auc:0.975301\n",
      "[22:55:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3648 extra nodes, 472 pruned nodes, max_depth=18\n",
      "[433]\tvalidation_0-auc:0.975342\tvalidation_1-auc:0.975342\n",
      "[22:55:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3464 extra nodes, 406 pruned nodes, max_depth=18\n",
      "[434]\tvalidation_0-auc:0.975376\tvalidation_1-auc:0.975376\n",
      "[22:55:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2494 extra nodes, 294 pruned nodes, max_depth=18\n",
      "[435]\tvalidation_0-auc:0.975404\tvalidation_1-auc:0.975404\n",
      "[22:56:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2642 extra nodes, 438 pruned nodes, max_depth=18\n",
      "[436]\tvalidation_0-auc:0.975431\tvalidation_1-auc:0.975431\n",
      "[22:56:04] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3208 extra nodes, 414 pruned nodes, max_depth=18\n",
      "[437]\tvalidation_0-auc:0.975457\tvalidation_1-auc:0.975457\n",
      "[22:56:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3256 extra nodes, 386 pruned nodes, max_depth=18\n",
      "[438]\tvalidation_0-auc:0.975491\tvalidation_1-auc:0.975491\n",
      "[22:56:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2804 extra nodes, 288 pruned nodes, max_depth=18\n",
      "[439]\tvalidation_0-auc:0.975515\tvalidation_1-auc:0.975515\n",
      "[22:56:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3800 extra nodes, 414 pruned nodes, max_depth=18\n",
      "[440]\tvalidation_0-auc:0.975532\tvalidation_1-auc:0.975532\n",
      "[22:56:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3654 extra nodes, 424 pruned nodes, max_depth=18\n",
      "[441]\tvalidation_0-auc:0.975565\tvalidation_1-auc:0.975565\n",
      "[22:56:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3648 extra nodes, 460 pruned nodes, max_depth=18\n",
      "[442]\tvalidation_0-auc:0.975596\tvalidation_1-auc:0.975596\n",
      "[22:56:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2374 extra nodes, 288 pruned nodes, max_depth=18\n",
      "[443]\tvalidation_0-auc:0.975621\tvalidation_1-auc:0.975621\n",
      "[22:56:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3164 extra nodes, 460 pruned nodes, max_depth=18\n",
      "[444]\tvalidation_0-auc:0.975652\tvalidation_1-auc:0.975652\n",
      "[22:56:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3092 extra nodes, 398 pruned nodes, max_depth=18\n",
      "[445]\tvalidation_0-auc:0.975683\tvalidation_1-auc:0.975683\n",
      "[22:56:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2014 extra nodes, 264 pruned nodes, max_depth=18\n",
      "[446]\tvalidation_0-auc:0.975689\tvalidation_1-auc:0.975689\n",
      "[22:56:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2490 extra nodes, 314 pruned nodes, max_depth=18\n",
      "[447]\tvalidation_0-auc:0.975713\tvalidation_1-auc:0.975713\n",
      "[22:56:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 1658 extra nodes, 228 pruned nodes, max_depth=18\n",
      "[448]\tvalidation_0-auc:0.975721\tvalidation_1-auc:0.975721\n",
      "[22:56:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3150 extra nodes, 372 pruned nodes, max_depth=18\n",
      "[449]\tvalidation_0-auc:0.975755\tvalidation_1-auc:0.975755\n",
      "[22:56:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2624 extra nodes, 380 pruned nodes, max_depth=18\n",
      "[450]\tvalidation_0-auc:0.975779\tvalidation_1-auc:0.975779\n",
      "[22:56:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 1652 extra nodes, 286 pruned nodes, max_depth=18\n",
      "[451]\tvalidation_0-auc:0.975789\tvalidation_1-auc:0.975789\n",
      "[22:56:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2804 extra nodes, 296 pruned nodes, max_depth=18\n",
      "[452]\tvalidation_0-auc:0.975807\tvalidation_1-auc:0.975807\n",
      "[22:56:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3290 extra nodes, 436 pruned nodes, max_depth=18\n",
      "[453]\tvalidation_0-auc:0.975828\tvalidation_1-auc:0.975828\n",
      "[22:57:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3438 extra nodes, 444 pruned nodes, max_depth=18\n",
      "[454]\tvalidation_0-auc:0.975868\tvalidation_1-auc:0.975868\n",
      "[22:57:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3132 extra nodes, 426 pruned nodes, max_depth=18\n",
      "[455]\tvalidation_0-auc:0.975888\tvalidation_1-auc:0.975888\n",
      "[22:57:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4120 extra nodes, 536 pruned nodes, max_depth=18\n",
      "[456]\tvalidation_0-auc:0.975922\tvalidation_1-auc:0.975922\n",
      "[22:57:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3348 extra nodes, 362 pruned nodes, max_depth=18\n",
      "[457]\tvalidation_0-auc:0.975958\tvalidation_1-auc:0.975958\n",
      "[22:57:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3218 extra nodes, 386 pruned nodes, max_depth=18\n",
      "[458]\tvalidation_0-auc:0.975988\tvalidation_1-auc:0.975988\n",
      "[22:57:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3710 extra nodes, 412 pruned nodes, max_depth=18\n",
      "[459]\tvalidation_0-auc:0.976023\tvalidation_1-auc:0.976023\n",
      "[22:57:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2972 extra nodes, 404 pruned nodes, max_depth=18\n",
      "[460]\tvalidation_0-auc:0.976048\tvalidation_1-auc:0.976048\n",
      "[22:57:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2912 extra nodes, 366 pruned nodes, max_depth=18\n",
      "[461]\tvalidation_0-auc:0.976077\tvalidation_1-auc:0.976077\n",
      "[22:57:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2464 extra nodes, 336 pruned nodes, max_depth=18\n",
      "[462]\tvalidation_0-auc:0.976091\tvalidation_1-auc:0.976091\n",
      "[22:57:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2470 extra nodes, 328 pruned nodes, max_depth=18\n",
      "[463]\tvalidation_0-auc:0.976107\tvalidation_1-auc:0.976107\n",
      "[22:57:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3634 extra nodes, 540 pruned nodes, max_depth=18\n",
      "[464]\tvalidation_0-auc:0.976131\tvalidation_1-auc:0.976131\n",
      "[22:57:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3754 extra nodes, 488 pruned nodes, max_depth=18\n",
      "[465]\tvalidation_0-auc:0.97617\tvalidation_1-auc:0.97617\n",
      "[22:57:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3532 extra nodes, 494 pruned nodes, max_depth=18\n",
      "[466]\tvalidation_0-auc:0.976209\tvalidation_1-auc:0.976209\n",
      "[22:57:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2854 extra nodes, 342 pruned nodes, max_depth=18\n",
      "[467]\tvalidation_0-auc:0.976227\tvalidation_1-auc:0.976227\n",
      "[22:57:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2890 extra nodes, 442 pruned nodes, max_depth=18\n",
      "[468]\tvalidation_0-auc:0.976247\tvalidation_1-auc:0.976247\n",
      "[22:57:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3024 extra nodes, 378 pruned nodes, max_depth=18\n",
      "[469]\tvalidation_0-auc:0.976277\tvalidation_1-auc:0.976277\n",
      "[22:57:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3102 extra nodes, 400 pruned nodes, max_depth=18\n",
      "[470]\tvalidation_0-auc:0.976303\tvalidation_1-auc:0.976303\n",
      "[22:57:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2536 extra nodes, 354 pruned nodes, max_depth=18\n",
      "[471]\tvalidation_0-auc:0.976321\tvalidation_1-auc:0.976321\n",
      "[22:58:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2636 extra nodes, 356 pruned nodes, max_depth=18\n",
      "[472]\tvalidation_0-auc:0.976344\tvalidation_1-auc:0.976344\n",
      "[22:58:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2860 extra nodes, 372 pruned nodes, max_depth=18\n",
      "[473]\tvalidation_0-auc:0.976367\tvalidation_1-auc:0.976367\n",
      "[22:58:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3446 extra nodes, 480 pruned nodes, max_depth=18\n",
      "[474]\tvalidation_0-auc:0.976397\tvalidation_1-auc:0.976397\n",
      "[22:58:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3262 extra nodes, 380 pruned nodes, max_depth=18\n",
      "[475]\tvalidation_0-auc:0.97643\tvalidation_1-auc:0.97643\n",
      "[22:58:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2258 extra nodes, 392 pruned nodes, max_depth=18\n",
      "[476]\tvalidation_0-auc:0.976449\tvalidation_1-auc:0.976449\n",
      "[22:58:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3310 extra nodes, 454 pruned nodes, max_depth=18\n",
      "[477]\tvalidation_0-auc:0.976477\tvalidation_1-auc:0.976477\n",
      "[22:58:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2848 extra nodes, 322 pruned nodes, max_depth=18\n",
      "[478]\tvalidation_0-auc:0.976482\tvalidation_1-auc:0.976482\n",
      "[22:58:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2948 extra nodes, 412 pruned nodes, max_depth=18\n",
      "[479]\tvalidation_0-auc:0.976502\tvalidation_1-auc:0.976502\n",
      "[22:58:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3538 extra nodes, 436 pruned nodes, max_depth=18\n",
      "[480]\tvalidation_0-auc:0.97654\tvalidation_1-auc:0.97654\n",
      "[22:58:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2376 extra nodes, 306 pruned nodes, max_depth=18\n",
      "[481]\tvalidation_0-auc:0.976556\tvalidation_1-auc:0.976556\n",
      "[22:58:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3126 extra nodes, 400 pruned nodes, max_depth=18\n",
      "[482]\tvalidation_0-auc:0.976577\tvalidation_1-auc:0.976577\n",
      "[22:58:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2368 extra nodes, 368 pruned nodes, max_depth=18\n",
      "[483]\tvalidation_0-auc:0.976586\tvalidation_1-auc:0.976586\n",
      "[22:58:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2970 extra nodes, 334 pruned nodes, max_depth=18\n",
      "[484]\tvalidation_0-auc:0.976617\tvalidation_1-auc:0.976617\n",
      "[22:58:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 1200 extra nodes, 144 pruned nodes, max_depth=18\n",
      "[485]\tvalidation_0-auc:0.976623\tvalidation_1-auc:0.976623\n",
      "[22:58:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3266 extra nodes, 446 pruned nodes, max_depth=18\n",
      "[486]\tvalidation_0-auc:0.976645\tvalidation_1-auc:0.976645\n",
      "[22:58:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3594 extra nodes, 450 pruned nodes, max_depth=18\n",
      "[487]\tvalidation_0-auc:0.976677\tvalidation_1-auc:0.976677\n",
      "[22:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3032 extra nodes, 474 pruned nodes, max_depth=18\n",
      "[488]\tvalidation_0-auc:0.976704\tvalidation_1-auc:0.976704\n",
      "[22:59:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3328 extra nodes, 434 pruned nodes, max_depth=18\n",
      "[489]\tvalidation_0-auc:0.976728\tvalidation_1-auc:0.976728\n",
      "[22:59:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4380 extra nodes, 542 pruned nodes, max_depth=18\n",
      "[490]\tvalidation_0-auc:0.976764\tvalidation_1-auc:0.976764\n",
      "[22:59:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2172 extra nodes, 276 pruned nodes, max_depth=18\n",
      "[491]\tvalidation_0-auc:0.976781\tvalidation_1-auc:0.976781\n",
      "[22:59:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2280 extra nodes, 328 pruned nodes, max_depth=18\n",
      "[492]\tvalidation_0-auc:0.976796\tvalidation_1-auc:0.976796\n",
      "[22:59:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2240 extra nodes, 330 pruned nodes, max_depth=18\n",
      "[493]\tvalidation_0-auc:0.976812\tvalidation_1-auc:0.976812\n",
      "[22:59:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3842 extra nodes, 546 pruned nodes, max_depth=18\n",
      "[494]\tvalidation_0-auc:0.976849\tvalidation_1-auc:0.976849\n",
      "[22:59:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2586 extra nodes, 364 pruned nodes, max_depth=18\n",
      "[495]\tvalidation_0-auc:0.976865\tvalidation_1-auc:0.976865\n",
      "[22:59:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2874 extra nodes, 408 pruned nodes, max_depth=18\n",
      "[496]\tvalidation_0-auc:0.976883\tvalidation_1-auc:0.976883\n",
      "[22:59:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3032 extra nodes, 444 pruned nodes, max_depth=18\n",
      "[497]\tvalidation_0-auc:0.976897\tvalidation_1-auc:0.976897\n",
      "[22:59:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2728 extra nodes, 350 pruned nodes, max_depth=18\n",
      "[498]\tvalidation_0-auc:0.976916\tvalidation_1-auc:0.976916\n",
      "[22:59:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2764 extra nodes, 364 pruned nodes, max_depth=18\n",
      "[499]\tvalidation_0-auc:0.976929\tvalidation_1-auc:0.976929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-25 22:59:36,209  <ipython-input-49-af38504e9331> : INFO  train finish\n"
     ]
    }
   ],
   "source": [
    "model = xgb.sklearn.XGBClassifier(\n",
    "    nthread=4,\n",
    "    learn_rate=0.17,\n",
    "    max_depth=18,\n",
    "    min_child_weight=1.1,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    colsample_bylevel=0.7,\n",
    "    objective='rank:pairwise',\n",
    "    n_estimators=500,\n",
    "    gamma=0.1,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=1,\n",
    "    max_delta_step=0,\n",
    "    scale_pos_weight=1,\n",
    "    silent=True\n",
    ")\n",
    "watchlist = [(train_dataset_x, train_dataset_y), (valid_dataset_x, valid_dataset_y)]\n",
    "\n",
    "logging.info('train begin')\n",
    "model.fit(train_dataset_x, train_dataset_y, eval_set=watchlist, eval_metric='auc', early_stopping_rounds=10)\n",
    "logging.info('train finish')\n",
    "\n",
    "model.save_model('../model/xgb.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leewind/.local/share/virtualenvs/leewind-p6XO93Th/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9381759179762218"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_test_prob_y = model.predict_proba(valid_dataset_x)\n",
    "model_test_df['Probability'] = predict_test_prob_y[:, 1]\n",
    "evaluate(model_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # gbtree and dart use tree based models while gblinear uses linear functions.\n",
    "    'booster': 'gbtree',\n",
    "    #  Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized\n",
    "    'objective': 'rank:pairwise',\n",
    "    # auc: Area under the curve\n",
    "    'eval_metric': 'auc',\n",
    "    # Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n",
    "    'gamma': 0.1,\n",
    "    'min_child_weight': 1.1,\n",
    "    \n",
    "    'max_depth':4,\n",
    "#     'max_depth': 12,\n",
    "    # Maximum number of nodes to be added\n",
    "    'max_leaves': 128,\n",
    "    # L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "    'lambda': 3,\n",
    "    \n",
    "    'alpha': 2,\n",
    "    # Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \n",
    "    'subsample': 0.7,\n",
    "    # This is a family of parameters for subsampling of columns.\n",
    "    'colsample_bytree': 0.7,\n",
    "    'colsample_bylevel': 0.7,\n",
    "    # learning_rate\n",
    "    'eta': 0.01,\n",
    "    # Exact greedy algorithm\n",
    "    'tree_method': 'exact',\n",
    "    # Random number seed.\n",
    "    'seed': 0,\n",
    "    'nthread': 4,\n",
    "    # Verbosity of printing messages. Valid values are 0 (silent),\n",
    "    'verbosity': 0,\n",
    "    'metric_freq': 100,\n",
    "}\n",
    "\n",
    "watchlist = [(xgbtrain, 'train'), (xgbvalid, 'validate')]\n",
    "\n",
    "logging.info('train begin')\n",
    "model = xgb.train(params, xgbtrain, num_boost_round=200, evals=watchlist)\n",
    "logging.info('train end')\n",
    "model.save_model('../model/xgb.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB调参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDT 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skl_min_n_estimators = 10\n",
    "skl_max_n_estimators = 500\n",
    "skl_n_estimators_step = 10\n",
    "skl_n_jobs = 2\n",
    "skl_random_seed = 2018\n",
    "\n",
    "## random forest tree classifier\n",
    "space = {\n",
    "    'n_estimators': hp.quniform(\"n_estimators\", skl_min_n_estimators, skl_max_n_estimators, skl_n_estimators_step),\n",
    "    'learning_rate': hp.quniform(\"learning_rate\", 0.01, 0.5, 0.01),\n",
    "    'max_features': hp.quniform(\"max_features\", 0.05, 1.0, 0.05),\n",
    "    'max_depth': hp.quniform('max_depth', 1, 15, 1),\n",
    "    'subsample': hp.quniform('subsample', 0.5, 1, 0.1),\n",
    "    'random_state': skl_random_seed\n",
    "}\n",
    "\n",
    "def objective(params):\n",
    "    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Tuning\"\"\"\n",
    "    \n",
    "    logger.info(params)\n",
    "    \n",
    "    gbcf = GradientBoostingClassifier(\n",
    "        n_estimators=int(params['n_estimators']), \n",
    "        max_features=params['max_features'], \n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_depth=params['max_depth'], \n",
    "        subsample=params['subsample'],\n",
    "        random_state=params['random_state']\n",
    "    )\n",
    "    \n",
    "    gbcf.fit(train_dataset_x, train_dataset_y)\n",
    "    \n",
    "    predict_test_prob_y = gbcf.predict_proba(valid_dataset_x)\n",
    "    model_test_df['Probability'] = predict_test_prob_y[:, 1]\n",
    "    \n",
    "    score = evaluate(model_test_df)\n",
    "\n",
    "#     gbdt = GradientBoostingRegressor(\n",
    "#         n_estimators=int(params['n_estimators']), \n",
    "#         max_features=params['max_features'], \n",
    "#         learning_rate=params['learning_rate'],\n",
    "#         max_depth=params['max_depth'], \n",
    "#         subsample=params['subsample'],\n",
    "#         random_state=params['random_state'],\n",
    "#         verbose=1\n",
    "#     )\n",
    "    \n",
    "#     gbdt.fit(train_dataset_x, train_dataset_y)\n",
    "#     score = gbdt.score(valid_dataset_x, valid_dataset_y)\n",
    "    logging.info('Socre is %f' % score)\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    loss = 1 - score\n",
    "    \n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "\n",
    "def evaluate(result_df):\n",
    "    group = result_df.groupby(['Coupon_id'])\n",
    "    aucs = []\n",
    "    for i in group:\n",
    "        tmpdf = i[1]        \n",
    "        if len(tmpdf['Is_in_day_consume'].unique()) != 2:\n",
    "            continue\n",
    "            \n",
    "        fpr, tpr, thresholds = roc_curve(tmpdf['Is_in_day_consume'], tmpdf['Probability'], pos_label=1)\n",
    "        auc_score = auc(fpr,tpr)\n",
    "        aucs.append(auc_score)\n",
    "            \n",
    "    return np.average(aucs)\n",
    "\n",
    "MAX_EVALS = 500\n",
    "\n",
    "# Optimize\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = Trials())\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred_df = pd.read_csv('../features/lcm_submit_features.csv')\n",
    "\n",
    "model_pred_df = pd.merge(model_pred_df, get_factor(user_features_df, 'User_id', 'User'), on=['User_id'], how='left')\n",
    "model_pred_df = pd.merge(model_pred_df, get_factor(merchant_features_df, 'Merchant_id', 'Merchant'), on=['Merchant_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>Merchant_id</th>\n",
       "      <th>Coupon_id</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Date_received</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Base_consume</th>\n",
       "      <th>Discount_money</th>\n",
       "      <th>Day_in_month</th>\n",
       "      <th>Day_in_week</th>\n",
       "      <th>Coupon_type</th>\n",
       "      <th>User_factor_alpha</th>\n",
       "      <th>User_factor_beta</th>\n",
       "      <th>Merchant_factor_alpha</th>\n",
       "      <th>Merchant_factor_beta</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43671</th>\n",
       "      <td>2751537</td>\n",
       "      <td>7910.0</td>\n",
       "      <td>2637.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20160702.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.395742</td>\n",
       "      <td>0.057898</td>\n",
       "      <td>0.006350</td>\n",
       "      <td>0.329149</td>\n",
       "      <td>6.973665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43669</th>\n",
       "      <td>2751537</td>\n",
       "      <td>7910.0</td>\n",
       "      <td>2637.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20160702.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.395742</td>\n",
       "      <td>0.057898</td>\n",
       "      <td>0.006350</td>\n",
       "      <td>0.329149</td>\n",
       "      <td>6.973665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47968</th>\n",
       "      <td>7294555</td>\n",
       "      <td>6135.0</td>\n",
       "      <td>8182.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20160712.0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.117911</td>\n",
       "      <td>0.081931</td>\n",
       "      <td>0.006191</td>\n",
       "      <td>0.330748</td>\n",
       "      <td>6.083786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104469</th>\n",
       "      <td>1535039</td>\n",
       "      <td>6135.0</td>\n",
       "      <td>8182.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20160712.0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.117266</td>\n",
       "      <td>0.081960</td>\n",
       "      <td>0.006191</td>\n",
       "      <td>0.330748</td>\n",
       "      <td>6.053985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45934</th>\n",
       "      <td>3977895</td>\n",
       "      <td>4808.0</td>\n",
       "      <td>1226.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20160709.0</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.050597</td>\n",
       "      <td>0.090049</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.314645</td>\n",
       "      <td>5.846001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        User_id  Merchant_id  Coupon_id  Distance  Date_received  Discount  \\\n",
       "43671   2751537       7910.0     2637.0       0.0     20160702.0  0.833333   \n",
       "43669   2751537       7910.0     2637.0       0.0     20160702.0  0.833333   \n",
       "47968   7294555       6135.0     8182.0       0.0     20160712.0  0.900000   \n",
       "104469  1535039       6135.0     8182.0       0.0     20160712.0  0.900000   \n",
       "45934   3977895       4808.0     1226.0       0.0     20160709.0  0.950000   \n",
       "\n",
       "        Base_consume  Discount_money  Day_in_month  Day_in_week  Coupon_type  \\\n",
       "43671           30.0             5.0           2.0          6.0          1.0   \n",
       "43669           30.0             5.0           2.0          6.0          1.0   \n",
       "47968           10.0             1.0          12.0          2.0          1.0   \n",
       "104469          10.0             1.0          12.0          2.0          1.0   \n",
       "45934           20.0             1.0           9.0          6.0          1.0   \n",
       "\n",
       "        User_factor_alpha  User_factor_beta  Merchant_factor_alpha  \\\n",
       "43671            0.395742          0.057898               0.006350   \n",
       "43669            0.395742          0.057898               0.006350   \n",
       "47968            0.117911          0.081931               0.006191   \n",
       "104469           0.117266          0.081960               0.006191   \n",
       "45934            0.050597          0.090049               0.000071   \n",
       "\n",
       "        Merchant_factor_beta  Probability  \n",
       "43671               0.329149     6.973665  \n",
       "43669               0.329149     6.973665  \n",
       "47968               0.330748     6.083786  \n",
       "104469              0.330748     6.053985  \n",
       "45934               0.314645     5.846001  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_dataset_x = fp.transform(model_pred_df[fields+continous])\n",
    "predict_prob_y = model.predict_proba(predict_dataset_x)\n",
    "model_pred_df['Probability'] = predict_prob_y[:, 1]\n",
    "model_pred_df.sort_values(['Probability'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113640, 4)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result_df = model_pred_df[['User_id', 'Coupon_id', 'Date_received', 'Probability']]\n",
    "final_result_df.to_csv('/Users/leewind/Desktop/submission_20190126.csv', index=False, header=False)\n",
    "final_result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>Coupon_id</th>\n",
       "      <th>Date_received</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.136400e+05</td>\n",
       "      <td>113640.000000</td>\n",
       "      <td>1.136400e+05</td>\n",
       "      <td>113640.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.684858e+06</td>\n",
       "      <td>9053.810929</td>\n",
       "      <td>2.016072e+07</td>\n",
       "      <td>-2.091856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.126259e+06</td>\n",
       "      <td>4145.873088</td>\n",
       "      <td>9.019508e+00</td>\n",
       "      <td>1.865192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.090000e+02</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.016070e+07</td>\n",
       "      <td>-9.081948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.844191e+06</td>\n",
       "      <td>5023.000000</td>\n",
       "      <td>2.016071e+07</td>\n",
       "      <td>-3.309978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.683266e+06</td>\n",
       "      <td>9983.000000</td>\n",
       "      <td>2.016072e+07</td>\n",
       "      <td>-2.050517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.525845e+06</td>\n",
       "      <td>13602.000000</td>\n",
       "      <td>2.016072e+07</td>\n",
       "      <td>-0.762426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.361024e+06</td>\n",
       "      <td>14045.000000</td>\n",
       "      <td>2.016073e+07</td>\n",
       "      <td>6.973665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            User_id      Coupon_id  Date_received    Probability\n",
       "count  1.136400e+05  113640.000000   1.136400e+05  113640.000000\n",
       "mean   3.684858e+06    9053.810929   2.016072e+07      -2.091856\n",
       "std    2.126259e+06    4145.873088   9.019508e+00       1.865192\n",
       "min    2.090000e+02       3.000000   2.016070e+07      -9.081948\n",
       "25%    1.844191e+06    5023.000000   2.016071e+07      -3.309978\n",
       "50%    3.683266e+06    9983.000000   2.016072e+07      -2.050517\n",
       "75%    5.525845e+06   13602.000000   2.016072e+07      -0.762426\n",
       "max    7.361024e+06   14045.000000   2.016073e+07       6.973665"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
