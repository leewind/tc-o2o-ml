{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer, StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from mlxtend.preprocessing import DenseTransformer\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('ai')\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(asctime)s  %(filename)s : %(levelname)s  %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "+ [Feature transformations with ensembles of trees](https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html)\n",
    "+ [sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "+ [机器学习之 sklearn中的pipeline](http://frankchen.xyz/2018/04/08/pipeline-in-machine-learning/)\n",
    "    - 使用pipeline做cross validation\n",
    "    - 自定义transformer\n",
    "    - FeatureUnion\n",
    "+ [Concatenating multiple feature extraction methods](https://scikit-learn.org/stable/auto_examples/compose/plot_feature_union.html#sphx-glr-auto-examples-compose-plot-feature-union-py)\n",
    "+ [sklearn.pipeline.Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "+ [gbdt+lr demo](https://github.com/princewen/tensorflow_practice/blob/master/recommendation/GBDT%2BLR-Demo/GBDT_LR.py)\n",
    "+ [推荐系统遇上深度学习(十)--GBDT+LR融合方案实战](https://zhuanlan.zhihu.com/p/37522339)\n",
    "+ [python︱sklearn一些小技巧的记录（训练集划分/pipelline/交叉验证等）](https://blog.csdn.net/sinat_26917383/article/details/77917881)\n",
    "+ [16.【进阶】特征提升之特征筛选----feature_selection](https://blog.csdn.net/jh1137921986/article/details/79822512)\n",
    "+ [使用sklearn优雅地进行数据挖掘](https://www.cnblogs.com/jasonfreak/p/5448462.html)\n",
    "+ [Kaggle机器学习之模型融合（stacking）心得](https://zhuanlan.zhihu.com/p/26890738)\n",
    "+ [model_library_config](https://github.com/ChenglongChen/Kaggle_CrowdFlower/blob/master/Code/Model/model_library_config.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "continous = [\n",
    "    'Discount',\n",
    "    'Previous_duration',\n",
    "    'Next_duration',\n",
    "    'Base_consume',\n",
    "    'User_receive_count',\n",
    "    'User_consume_count',\n",
    "    'User_used_count',\n",
    "    'User_not_used_count',\n",
    "    'User_used_coupon_rate',\n",
    "    'User_used_coupon_rate_max',\n",
    "    'User_used_coupon_rate_min',\n",
    "    'User_used_coupon_rate_mean',\n",
    "    'User_receive_coupon_merchant_count',\n",
    "    'User_consume_merchant_count',\n",
    "    'User_used_coupon_merchant_count',\n",
    "    'User_used_coupon_merchant_occ',\n",
    "    'User_receive_different_coupon_count',\n",
    "    'User_used_different_coupon_count',\n",
    "    'User_receive_different_coupon_occ',\n",
    "    'User_used_different_coupon_occ',\n",
    "    'User_receive_coupon_mean',\n",
    "    'User_used_coupon_mean',\n",
    "    'User_distance_used_mean',\n",
    "    'User_distance_used_max',\n",
    "    'User_distance_used_min',\n",
    "    'User_duration_used_mean',\n",
    "    'User_duration_used_max',\n",
    "    'User_duration_used_min',\n",
    "    'User_previous_duration_used_mean',\n",
    "    'User_previous_duration_used_max',\n",
    "    'User_previous_duration_used_min',\n",
    "    'User_next_duration_used_mean',\n",
    "    'User_next_duration_used_max',\n",
    "    'User_next_duration_used_min',\n",
    "    'Merchant_receive_count',\n",
    "    'Merchant_consume_count',\n",
    "    'Merchant_used_count',\n",
    "    'Merchant_not_used_count',\n",
    "    'Merchant_used_coupon_rate',\n",
    "    'Merchant_used_coupon_rate_max',\n",
    "    'Merchant_used_coupon_rate_min',\n",
    "    'Merchant_used_coupon_rate_mean',\n",
    "    'Merchant_receive_coupon_user_count',\n",
    "    'Merchant_consume_user_count',\n",
    "    'Merchant_used_coupon_user_count',\n",
    "    'Merchant_receive_coupon_user_occ',\n",
    "    'Merchant_consume_user_occ',\n",
    "    'Merchant_used_coupon_user_occ',\n",
    "    'Merchant_receive_different_coupon_count',\n",
    "    'Merchant_used_different_coupon_count',\n",
    "    'Merchant_receive_different_coupon_occ',\n",
    "    'Merchant_used_different_coupon_occ',\n",
    "    'Merchant_receive_coupon_mean',\n",
    "    'Merchant_used_coupon_mean',\n",
    "    'Merchant_receive_different_coupon_avg',\n",
    "    'Merchant_used_different_coupon_avg',\n",
    "    'Merchant_distance_used_mean',\n",
    "    'Merchant_distance_used_max',\n",
    "    'Merchant_distance_used_min',\n",
    "    'Merchant_duration_used_mean',\n",
    "    'Merchant_duration_used_max',\n",
    "    'Merchant_duration_used_min',\n",
    "    'Merchant_previous_duration_used_mean',\n",
    "    'Merchant_previous_duration_used_max',\n",
    "    'Merchant_previous_duration_used_min',\n",
    "    'Merchant_next_duration_used_mean',\n",
    "    'Merchant_next_duration_used_max',\n",
    "    'Merchant_next_duration_used_min',\n",
    "    'Coupon_received_count',\n",
    "    'Coupon_used_count',\n",
    "    'Coupon_used_rate',\n",
    "    'Coupon_duration_used_mean',\n",
    "    'Coupon_duration_used_max',\n",
    "    'Coupon_duration_used_min',\n",
    "    'Coupon_distance_used_mean',\n",
    "    'Coupon_distance_used_max',\n",
    "    'Coupon_distance_used_min',\n",
    "    'User_merchant_receive_count',\n",
    "    'User_merchant_consume_count',\n",
    "    'User_merchant_used_count',\n",
    "    'User_merchant_not_used_count',\n",
    "    'User_merchant_used_coupon_rate',\n",
    "    'User_merchant_not_used_coupon_rate',\n",
    "    'User_merchant_used_coupon_rate_4_merchant',\n",
    "    'User_merchant_not_used_coupon_rate_4_merchant',\n",
    "    'User_merchant_duration_used_mean',\n",
    "    'User_merchant_duration_used_max',\n",
    "    'User_merchant_duration_used_min',\n",
    "    'Online_user_receive_count',\n",
    "    'Online_user_consume_count',\n",
    "    'Online_user_used_count',\n",
    "    'Online_user_not_used_count',\n",
    "    'Online_user_used_coupon_rate',\n",
    "    'User_offline_consume_rate',\n",
    "    'User_offline_used_rate',\n",
    "    'User_offline_no_consume_coupon_rate',\n",
    "    'User_distance_receive_count',\n",
    "    'User_distance_consume_count',\n",
    "    'User_distance_used_count',\n",
    "    'User_distance_receive_rate',\n",
    "    'User_distance_consume_rate',\n",
    "    'User_distance_used_rate',\n",
    "    'User_coupon_type_receive_count',\n",
    "    'User_coupon_type_used_count',\n",
    "    'User_coupon_type_receive_rate',\n",
    "    'User_coupon_type_used_rate',\n",
    "    'User_coupon_receive_count',\n",
    "    'User_coupon_used_count',\n",
    "    'User_coupon_receive_rate',\n",
    "    'User_coupon_used_rate',\n",
    "    'Merchant_distance_receive_count',\n",
    "    'Merchant_distance_consume_count',\n",
    "    'Merchant_distance_used_count',\n",
    "    'Merchant_distance_receive_rate',\n",
    "    'Merchant_distance_used_rate',\n",
    "    'User_coupon_duration_used_mean',\n",
    "    'User_coupon_duration_used_max',\n",
    "    'User_coupon_duration_used_min',\n",
    "    'User_received_date_count'\n",
    "]\n",
    "\n",
    "\n",
    "fields = [\n",
    "    'Distance',\n",
    "    'Day_in_month_received',\n",
    "    'Day_in_week_received',\n",
    "    'Coupon_type'\n",
    "]\n",
    "\n",
    "label = ['Is_in_day_consume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train_df = pd.read_csv('lcm_train_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test_df = pd.read_csv('lcm_train_test_features.csv')\n",
    "model_test_df = model_test_df[model_test_df['Coupon_id']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractFeature(TransformerMixin):\n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        return pd.DataFrame(X[:,0] * X[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('continuous', Pipeline([\n",
    "            ('extract', ColumnSelector(continous)),\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "            ('scale', Normalizer())\n",
    "        ])),\n",
    "        ('fields', Pipeline([\n",
    "            ('extract', ColumnSelector(fields)),\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan,  strategy='most_frequent')),\n",
    "            ('one_hot', OneHotEncoder(categories='auto')),\n",
    "            ('to_dense', DenseTransformer())\n",
    "        ])),\n",
    "        ('rate', Pipeline([\n",
    "            ('extract', ColumnSelector(['User_coupon_used_rate', 'User_used_coupon_rate'])),\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan,  strategy='most_frequent')),\n",
    "            ('new', ExtractFeature()),\n",
    "            ('scale', Normalizer())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('skb', SelectKBest(chi2, k=80)),\n",
    "    ('sc4gbdt', StandardScaler())\n",
    "])\n",
    "\n",
    "fp.fit(model_train_df[fields+continous], model_train_df[label].values.ravel())\n",
    "\n",
    "train_dataset_x = fp.transform(model_train_df[fields+continous])\n",
    "train_dataset_y = model_train_df[label].values.ravel()\n",
    "\n",
    "valid_dataset_x = fp.transform(model_test_df[fields+continous])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-19 19:58:17,046  tpe.py : INFO  tpe_transform took 0.006516 seconds\n",
      "2019-01-19 19:58:17,048  tpe.py : INFO  TPE using 0 trials\n",
      "2019-01-19 19:58:17,054  <ipython-input-20-ed20a3a385c0> : INFO  {'learning_rate': 0.21, 'max_depth': 12.0, 'max_features': 0.2, 'n_estimators': 210.0, 'random_state': 2018, 'subsample': 0.8}\n",
      "2019-01-19 21:43:04,931  <ipython-input-20-ed20a3a385c0> : INFO  Socre is 0.422708\n",
      "2019-01-19 21:43:04,956  tpe.py : INFO  tpe_transform took 0.006679 seconds\n",
      "2019-01-19 21:43:04,957  tpe.py : INFO  TPE using 1/1 trials with best loss 0.577292\n",
      "2019-01-19 21:43:04,961  <ipython-input-20-ed20a3a385c0> : INFO  {'learning_rate': 0.46, 'max_depth': 2.0, 'max_features': 0.6000000000000001, 'n_estimators': 150.0, 'random_state': 2018, 'subsample': 0.7000000000000001}\n",
      "2019-01-19 21:47:15,392  <ipython-input-20-ed20a3a385c0> : INFO  Socre is 0.493653\n",
      "2019-01-19 21:47:15,404  tpe.py : INFO  tpe_transform took 0.002821 seconds\n",
      "2019-01-19 21:47:15,407  tpe.py : INFO  TPE using 2/2 trials with best loss 0.506347\n",
      "2019-01-19 21:47:15,411  <ipython-input-20-ed20a3a385c0> : INFO  {'learning_rate': 0.16, 'max_depth': 9.0, 'max_features': 0.1, 'n_estimators': 410.0, 'random_state': 2018, 'subsample': 0.7000000000000001}\n",
      "2019-01-19 22:45:16,558  <ipython-input-20-ed20a3a385c0> : INFO  Socre is 0.418366\n",
      "2019-01-19 22:45:16,570  tpe.py : INFO  tpe_transform took 0.002935 seconds\n",
      "2019-01-19 22:45:16,571  tpe.py : INFO  TPE using 3/3 trials with best loss 0.506347\n",
      "2019-01-19 22:45:16,574  <ipython-input-20-ed20a3a385c0> : INFO  {'learning_rate': 0.32, 'max_depth': 5.0, 'max_features': 0.6000000000000001, 'n_estimators': 230.0, 'random_state': 2018, 'subsample': 0.9}\n",
      "2019-01-19 23:09:34,844  <ipython-input-20-ed20a3a385c0> : INFO  Socre is 0.495734\n",
      "2019-01-19 23:09:34,855  tpe.py : INFO  tpe_transform took 0.002824 seconds\n",
      "2019-01-19 23:09:34,856  tpe.py : INFO  TPE using 4/4 trials with best loss 0.504266\n",
      "2019-01-19 23:09:34,861  <ipython-input-20-ed20a3a385c0> : INFO  {'learning_rate': 0.12, 'max_depth': 8.0, 'max_features': 0.30000000000000004, 'n_estimators': 230.0, 'random_state': 2018, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "skl_min_n_estimators = 10\n",
    "skl_max_n_estimators = 500\n",
    "skl_n_estimators_step = 10\n",
    "skl_n_jobs = 2\n",
    "skl_random_seed = 2018\n",
    "\n",
    "## random forest tree classifier\n",
    "space = {\n",
    "    'n_estimators': hp.quniform(\"n_estimators\", skl_min_n_estimators, skl_max_n_estimators, skl_n_estimators_step),\n",
    "    'learning_rate': hp.quniform(\"learning_rate\", 0.01, 0.5, 0.01),\n",
    "    'max_features': hp.quniform(\"max_features\", 0.05, 1.0, 0.05),\n",
    "    'max_depth': hp.quniform('max_depth', 1, 15, 1),\n",
    "    'subsample': hp.quniform('subsample', 0.5, 1, 0.1),\n",
    "    'random_state': skl_random_seed\n",
    "}\n",
    "\n",
    "def objective(params):\n",
    "    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Tuning\"\"\"\n",
    "    \n",
    "    logger.info(params)\n",
    "    \n",
    "    gbcf = GradientBoostingClassifier(\n",
    "        n_estimators=int(params['n_estimators']), \n",
    "        max_features=params['max_features'], \n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_depth=params['max_depth'], \n",
    "        subsample=params['subsample'],\n",
    "        random_state=params['random_state']\n",
    "    )\n",
    "    \n",
    "    gbcf.fit(train_dataset_x, train_dataset_y)\n",
    "    \n",
    "    predict_test_prob_y = gbcf.predict_proba(valid_dataset_x)\n",
    "    model_test_df['Probability'] = predict_test_prob_y[:, 1]\n",
    "    \n",
    "    score = evaluate(model_test_df)\n",
    "    logging.info('Socre is %f' % score)\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    loss = 1 - score\n",
    "    \n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "\n",
    "def evaluate(result_df):\n",
    "    group = result_df.groupby(['Coupon_id'])\n",
    "    aucs = []\n",
    "    for i in group:\n",
    "        tmpdf = i[1]        \n",
    "        if len(tmpdf['Is_in_day_consume'].unique()) != 2:\n",
    "            continue\n",
    "            \n",
    "        fpr, tpr, thresholds = roc_curve(tmpdf['Is_in_day_consume'], tmpdf['Probability'], pos_label=1)\n",
    "        auc_score = auc(fpr,tpr)\n",
    "        aucs.append(auc_score)\n",
    "            \n",
    "    return np.average(aucs)\n",
    "\n",
    "MAX_EVALS = 500\n",
    "\n",
    "# Optimize\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = Trials())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>Merchant_id</th>\n",
       "      <th>Coupon_id</th>\n",
       "      <th>Discount_rate</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Date_received</th>\n",
       "      <th>Previous_date_received</th>\n",
       "      <th>Next_date_received</th>\n",
       "      <th>Previous_duration</th>\n",
       "      <th>Next_duration</th>\n",
       "      <th>...</th>\n",
       "      <th>Merchant_distance_receive_count</th>\n",
       "      <th>Merchant_distance_consume_count</th>\n",
       "      <th>Merchant_distance_used_count</th>\n",
       "      <th>Merchant_distance_receive_rate</th>\n",
       "      <th>Merchant_distance_used_rate</th>\n",
       "      <th>User_coupon_duration_used_mean</th>\n",
       "      <th>User_coupon_duration_used_max</th>\n",
       "      <th>User_coupon_duration_used_min</th>\n",
       "      <th>User_received_date_count</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92750</th>\n",
       "      <td>6013165</td>\n",
       "      <td>7963</td>\n",
       "      <td>5548</td>\n",
       "      <td>20:1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20160723</td>\n",
       "      <td>20160722.0</td>\n",
       "      <td>20160728.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92751</th>\n",
       "      <td>6013165</td>\n",
       "      <td>7963</td>\n",
       "      <td>5548</td>\n",
       "      <td>20:1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20160728</td>\n",
       "      <td>20160723.0</td>\n",
       "      <td>20160729.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92748</th>\n",
       "      <td>6013165</td>\n",
       "      <td>7963</td>\n",
       "      <td>5548</td>\n",
       "      <td>20:1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20160721</td>\n",
       "      <td>20160718.0</td>\n",
       "      <td>20160722.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92752</th>\n",
       "      <td>6013165</td>\n",
       "      <td>7963</td>\n",
       "      <td>5548</td>\n",
       "      <td>20:1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20160729</td>\n",
       "      <td>20160728.0</td>\n",
       "      <td>20160731.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92749</th>\n",
       "      <td>6013165</td>\n",
       "      <td>7963</td>\n",
       "      <td>5548</td>\n",
       "      <td>20:1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20160722</td>\n",
       "      <td>20160721.0</td>\n",
       "      <td>20160723.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       User_id  Merchant_id  Coupon_id Discount_rate  Distance  Date_received  \\\n",
       "92750  6013165         7963       5548          20:1       3.0       20160723   \n",
       "92751  6013165         7963       5548          20:1       3.0       20160728   \n",
       "92748  6013165         7963       5548          20:1       3.0       20160721   \n",
       "92752  6013165         7963       5548          20:1       3.0       20160729   \n",
       "92749  6013165         7963       5548          20:1       3.0       20160722   \n",
       "\n",
       "       Previous_date_received  Next_date_received  Previous_duration  \\\n",
       "92750              20160722.0          20160728.0                  2   \n",
       "92751              20160723.0          20160729.0                  6   \n",
       "92748              20160718.0          20160722.0                  4   \n",
       "92752              20160728.0          20160731.0                  2   \n",
       "92749              20160721.0          20160723.0                  2   \n",
       "\n",
       "       Next_duration     ...       Merchant_distance_receive_count  \\\n",
       "92750              6     ...                                   6.0   \n",
       "92751              2     ...                                   6.0   \n",
       "92748              2     ...                                   6.0   \n",
       "92752              3     ...                                   6.0   \n",
       "92749              2     ...                                   6.0   \n",
       "\n",
       "       Merchant_distance_consume_count  Merchant_distance_used_count  \\\n",
       "92750                             10.0                           3.0   \n",
       "92751                             10.0                           3.0   \n",
       "92748                             10.0                           3.0   \n",
       "92752                             10.0                           3.0   \n",
       "92749                             10.0                           3.0   \n",
       "\n",
       "       Merchant_distance_receive_rate  Merchant_distance_used_rate  \\\n",
       "92750                        0.315789                     0.157895   \n",
       "92751                        0.315789                     0.157895   \n",
       "92748                        0.315789                     0.157895   \n",
       "92752                        0.315789                     0.157895   \n",
       "92749                        0.315789                     0.157895   \n",
       "\n",
       "       User_coupon_duration_used_mean  User_coupon_duration_used_max  \\\n",
       "92750                        4.333333                            7.0   \n",
       "92751                        4.333333                            7.0   \n",
       "92748                        4.333333                            7.0   \n",
       "92752                        4.333333                            7.0   \n",
       "92749                        4.333333                            7.0   \n",
       "\n",
       "       User_coupon_duration_used_min  User_received_date_count  Probability  \n",
       "92750                            1.0                         7          1.0  \n",
       "92751                            1.0                         7          1.0  \n",
       "92748                            1.0                         7          1.0  \n",
       "92752                            1.0                         7          1.0  \n",
       "92749                            1.0                         7          1.0  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pred_df = pd.read_csv('lcm_test_features.csv')\n",
    "predict_prob_y = pipe_lr.predict_proba(model_pred_df[fields+continous])\n",
    "model_pred_df['Probability'] = predict_prob_y[:, 1]\n",
    "model_pred_df.sort_values(['Probability'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113640, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result_df = model_pred_df[['User_id', 'Coupon_id', 'Date_received', 'Probability']]\n",
    "final_result_df.to_csv('/Users/leewind/Desktop/submission_20190118.csv', index=False, header=False)\n",
    "final_result_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
