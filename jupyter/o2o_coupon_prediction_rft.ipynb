{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer, StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from mlxtend.preprocessing import DenseTransformer\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('ai')\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(asctime)s  %(filename)s : %(levelname)s  %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "+ [Feature transformations with ensembles of trees](https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html)\n",
    "+ [sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "+ [机器学习之 sklearn中的pipeline](http://frankchen.xyz/2018/04/08/pipeline-in-machine-learning/)\n",
    "    - 使用pipeline做cross validation\n",
    "    - 自定义transformer\n",
    "    - FeatureUnion\n",
    "+ [Concatenating multiple feature extraction methods](https://scikit-learn.org/stable/auto_examples/compose/plot_feature_union.html#sphx-glr-auto-examples-compose-plot-feature-union-py)\n",
    "+ [sklearn.pipeline.Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "+ [gbdt+lr demo](https://github.com/princewen/tensorflow_practice/blob/master/recommendation/GBDT%2BLR-Demo/GBDT_LR.py)\n",
    "+ [推荐系统遇上深度学习(十)--GBDT+LR融合方案实战](https://zhuanlan.zhihu.com/p/37522339)\n",
    "+ [python︱sklearn一些小技巧的记录（训练集划分/pipelline/交叉验证等）](https://blog.csdn.net/sinat_26917383/article/details/77917881)\n",
    "+ [16.【进阶】特征提升之特征筛选----feature_selection](https://blog.csdn.net/jh1137921986/article/details/79822512)\n",
    "+ [使用sklearn优雅地进行数据挖掘](https://www.cnblogs.com/jasonfreak/p/5448462.html)\n",
    "+ [Kaggle机器学习之模型融合（stacking）心得](https://zhuanlan.zhihu.com/p/26890738)\n",
    "+ [model_library_config](https://github.com/ChenglongChen/Kaggle_CrowdFlower/blob/master/Code/Model/model_library_config.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "continous = [\n",
    "    'Discount',\n",
    "    'Previous_duration',\n",
    "    'Next_duration',\n",
    "    'Base_consume',\n",
    "    'User_receive_count',\n",
    "    'User_consume_count',\n",
    "    'User_used_count',\n",
    "    'User_not_used_count',\n",
    "    'User_used_coupon_rate',\n",
    "    'User_used_coupon_rate_max',\n",
    "    'User_used_coupon_rate_min',\n",
    "    'User_used_coupon_rate_mean',\n",
    "    'User_receive_coupon_merchant_count',\n",
    "    'User_consume_merchant_count',\n",
    "    'User_used_coupon_merchant_count',\n",
    "    'User_used_coupon_merchant_occ',\n",
    "    'User_receive_different_coupon_count',\n",
    "    'User_used_different_coupon_count',\n",
    "    'User_receive_different_coupon_occ',\n",
    "    'User_used_different_coupon_occ',\n",
    "    'User_receive_coupon_mean',\n",
    "    'User_used_coupon_mean',\n",
    "    'User_distance_used_mean',\n",
    "    'User_distance_used_max',\n",
    "    'User_distance_used_min',\n",
    "    'User_duration_used_mean',\n",
    "    'User_duration_used_max',\n",
    "    'User_duration_used_min',\n",
    "    'User_previous_duration_used_mean',\n",
    "    'User_previous_duration_used_max',\n",
    "    'User_previous_duration_used_min',\n",
    "    'User_next_duration_used_mean',\n",
    "    'User_next_duration_used_max',\n",
    "    'User_next_duration_used_min',\n",
    "    'Merchant_receive_count',\n",
    "    'Merchant_consume_count',\n",
    "    'Merchant_used_count',\n",
    "    'Merchant_not_used_count',\n",
    "    'Merchant_used_coupon_rate',\n",
    "    'Merchant_used_coupon_rate_max',\n",
    "    'Merchant_used_coupon_rate_min',\n",
    "    'Merchant_used_coupon_rate_mean',\n",
    "    'Merchant_receive_coupon_user_count',\n",
    "    'Merchant_consume_user_count',\n",
    "    'Merchant_used_coupon_user_count',\n",
    "    'Merchant_receive_coupon_user_occ',\n",
    "    'Merchant_consume_user_occ',\n",
    "    'Merchant_used_coupon_user_occ',\n",
    "    'Merchant_receive_different_coupon_count',\n",
    "    'Merchant_used_different_coupon_count',\n",
    "    'Merchant_receive_different_coupon_occ',\n",
    "    'Merchant_used_different_coupon_occ',\n",
    "    'Merchant_receive_coupon_mean',\n",
    "    'Merchant_used_coupon_mean',\n",
    "    'Merchant_receive_different_coupon_avg',\n",
    "    'Merchant_used_different_coupon_avg',\n",
    "    'Merchant_distance_used_mean',\n",
    "    'Merchant_distance_used_max',\n",
    "    'Merchant_distance_used_min',\n",
    "    'Merchant_duration_used_mean',\n",
    "    'Merchant_duration_used_max',\n",
    "    'Merchant_duration_used_min',\n",
    "    'Merchant_previous_duration_used_mean',\n",
    "    'Merchant_previous_duration_used_max',\n",
    "    'Merchant_previous_duration_used_min',\n",
    "    'Merchant_next_duration_used_mean',\n",
    "    'Merchant_next_duration_used_max',\n",
    "    'Merchant_next_duration_used_min',\n",
    "    'Coupon_received_count',\n",
    "    'Coupon_used_count',\n",
    "    'Coupon_used_rate',\n",
    "    'Coupon_duration_used_mean',\n",
    "    'Coupon_duration_used_max',\n",
    "    'Coupon_duration_used_min',\n",
    "    'Coupon_distance_used_mean',\n",
    "    'Coupon_distance_used_max',\n",
    "    'Coupon_distance_used_min',\n",
    "    'User_merchant_receive_count',\n",
    "    'User_merchant_consume_count',\n",
    "    'User_merchant_used_count',\n",
    "    'User_merchant_not_used_count',\n",
    "    'User_merchant_used_coupon_rate',\n",
    "    'User_merchant_not_used_coupon_rate',\n",
    "    'User_merchant_used_coupon_rate_4_merchant',\n",
    "    'User_merchant_not_used_coupon_rate_4_merchant',\n",
    "    'User_merchant_duration_used_mean',\n",
    "    'User_merchant_duration_used_max',\n",
    "    'User_merchant_duration_used_min',\n",
    "    'Online_user_receive_count',\n",
    "    'Online_user_consume_count',\n",
    "    'Online_user_used_count',\n",
    "    'Online_user_not_used_count',\n",
    "    'Online_user_used_coupon_rate',\n",
    "    'User_offline_consume_rate',\n",
    "    'User_offline_used_rate',\n",
    "    'User_offline_no_consume_coupon_rate',\n",
    "    'User_distance_receive_count',\n",
    "    'User_distance_consume_count',\n",
    "    'User_distance_used_count',\n",
    "    'User_distance_receive_rate',\n",
    "    'User_distance_consume_rate',\n",
    "    'User_distance_used_rate',\n",
    "    'User_coupon_type_receive_count',\n",
    "    'User_coupon_type_used_count',\n",
    "    'User_coupon_type_receive_rate',\n",
    "    'User_coupon_type_used_rate',\n",
    "    'User_coupon_receive_count',\n",
    "    'User_coupon_used_count',\n",
    "    'User_coupon_receive_rate',\n",
    "    'User_coupon_used_rate',\n",
    "    'Merchant_distance_receive_count',\n",
    "    'Merchant_distance_consume_count',\n",
    "    'Merchant_distance_used_count',\n",
    "    'Merchant_distance_receive_rate',\n",
    "    'Merchant_distance_used_rate',\n",
    "    'User_coupon_duration_used_mean',\n",
    "    'User_coupon_duration_used_max',\n",
    "    'User_coupon_duration_used_min',\n",
    "    'User_received_date_count'\n",
    "]\n",
    "\n",
    "\n",
    "fields = [\n",
    "    'Distance',\n",
    "    'Day_in_month_received',\n",
    "    'Day_in_week_received',\n",
    "    'Coupon_type'\n",
    "]\n",
    "\n",
    "label = ['Duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train_df = pd.read_csv('../features/lcm_train_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test_df = pd.read_csv('../features/lcm_train_test_features.csv')\n",
    "model_test_df = model_test_df[model_test_df['Coupon_id']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractFeature(TransformerMixin):\n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        return pd.DataFrame(X[:,0] * X[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('continuous', Pipeline([\n",
    "            ('extract', ColumnSelector(continous)),\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "            ('scale', Normalizer())\n",
    "        ])),\n",
    "        ('fields', Pipeline([\n",
    "            ('extract', ColumnSelector(fields)),\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan,  strategy='most_frequent')),\n",
    "            ('one_hot', OneHotEncoder(categories='auto')),\n",
    "            ('to_dense', DenseTransformer())\n",
    "        ])),\n",
    "        ('rate', Pipeline([\n",
    "            ('extract', ColumnSelector(['User_coupon_used_rate', 'User_used_coupon_rate'])),\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan,  strategy='most_frequent')),\n",
    "            ('new', ExtractFeature()),\n",
    "            ('scale', Normalizer())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('skb', SelectKBest(chi2, k=80)),\n",
    "    ('sc4gbdt', StandardScaler())\n",
    "])\n",
    "\n",
    "fp.fit(model_train_df[fields+continous], model_train_df[label].values.ravel())\n",
    "\n",
    "train_dataset_x = fp.transform(model_train_df[fields+continous])\n",
    "train_dataset_y = model_train_df[label].values.ravel()\n",
    "\n",
    "valid_dataset_x = fp.transform(model_test_df[fields+continous])\n",
    "valid_dataset_y = model_test_df[label].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-22 00:23:22,690  tpe.py : INFO  tpe_transform took 0.010583 seconds\n",
      "2019-01-22 00:23:22,694  tpe.py : INFO  TPE using 0 trials\n",
      "2019-01-22 00:23:22,699  <ipython-input-8-e876bef2e5bb> : INFO  {'learning_rate': 0.42, 'max_depth': 5.0, 'max_features': 0.30000000000000004, 'n_estimators': 300.0, 'random_state': 2018, 'subsample': 0.7000000000000001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           3.6740           0.6724           19.18m\n",
      "         2           3.4188           0.2548           21.11m\n",
      "         3           3.2408           0.1060           21.87m\n",
      "         4           3.2237           0.0516           22.44m\n",
      "         5           3.1645           0.0380           22.32m\n",
      "         6           3.1819           0.0203           21.80m\n",
      "         7           3.1373           0.0227           21.58m\n",
      "         8           3.0643           0.0103           21.31m\n",
      "         9           3.1024           0.0057           21.16m\n",
      "        10           3.0486           0.0030           20.90m\n",
      "        20           2.9597           0.0026           19.63m\n",
      "        30           3.0018           0.0001           18.53m\n",
      "        40           2.9063          -0.0006           17.70m\n",
      "        50           2.9252          -0.0005           16.98m\n",
      "        60           2.8647          -0.0009           15.98m\n",
      "        70           2.8253          -0.0018           15.19m\n",
      "        80           2.8158          -0.0005           14.51m\n",
      "        90           2.7928          -0.0013           13.73m\n",
      "       100           2.7924          -0.0018           13.08m\n",
      "       200           2.6590          -0.0011            6.63m\n",
      "       300           2.5476           0.0000            0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-22 00:43:10,824  <ipython-input-8-e876bef2e5bb> : INFO  Socre is -16.502297\n",
      "2019-01-22 00:43:10,832  tpe.py : INFO  tpe_transform took 0.003673 seconds\n",
      "2019-01-22 00:43:10,833  tpe.py : INFO  TPE using 1/1 trials with best loss 17.502297\n",
      "2019-01-22 00:43:10,836  <ipython-input-8-e876bef2e5bb> : INFO  {'learning_rate': 0.18, 'max_depth': 5.0, 'max_features': 0.1, 'n_estimators': 330.0, 'random_state': 2018, 'subsample': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           3.9984            9.67m\n",
      "         2           3.7716            9.59m\n",
      "         3           3.6323            9.32m\n",
      "         4           3.5219            9.05m\n",
      "         5           3.4452            8.86m\n",
      "         6           3.3906            8.66m\n",
      "         7           3.3475            8.54m\n",
      "         8           3.3136            8.47m\n",
      "         9           3.2951            8.38m\n",
      "        10           3.2779            8.37m\n",
      "        20           3.1637            7.76m\n",
      "        30           3.1027            7.28m\n",
      "        40           3.0670            6.99m\n",
      "        50           3.0317            6.66m\n",
      "        60           3.0077            6.36m\n",
      "        70           2.9895            6.12m\n",
      "        80           2.9707            5.81m\n",
      "        90           2.9538            5.53m\n",
      "       100           2.9426            5.26m\n",
      "       200           2.8384            2.89m\n",
      "       300           2.7650           39.53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-22 00:50:38,257  <ipython-input-8-e876bef2e5bb> : INFO  Socre is -5.679980\n",
      "2019-01-22 00:50:38,264  tpe.py : INFO  tpe_transform took 0.003530 seconds\n",
      "2019-01-22 00:50:38,265  tpe.py : INFO  TPE using 2/2 trials with best loss 6.679980\n",
      "2019-01-22 00:50:38,268  <ipython-input-8-e876bef2e5bb> : INFO  {'learning_rate': 0.21, 'max_depth': 14.0, 'max_features': 0.4, 'n_estimators': 210.0, 'random_state': 2018, 'subsample': 0.5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           3.5608           0.4783          562.39m\n",
      "         2           3.2760           0.2905          602.21m\n",
      "         3           3.0018           0.1779          613.22m\n",
      "         4           2.8090           0.1025          681.43m\n",
      "         5           2.6674           0.0682          682.30m\n",
      "         6           2.5981           0.0275          664.37m\n",
      "         7           2.5795           0.0170          655.05m\n",
      "         8           2.4577           0.0100          640.52m\n",
      "         9           2.4562           0.0014          623.92m\n",
      "        10           2.3348          -0.0041          608.85m\n",
      "        20           2.1716          -0.0061          480.18m\n",
      "        30           2.1255          -0.0051          379.15m\n",
      "        40           1.9988          -0.0058          309.36m\n",
      "        50           1.9577          -0.0044          273.26m\n",
      "        60           1.8330          -0.0097          249.22m\n",
      "        70           1.7352          -0.0053          231.18m\n",
      "        80           1.6641          -0.0015          215.39m\n",
      "        90           1.5889          -0.0094          201.07m\n",
      "       100           1.5164          -0.0060          184.65m\n",
      "       200           1.0686          -0.0085           19.31m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-22 07:40:26,191  <ipython-input-8-e876bef2e5bb> : INFO  Socre is -10.068071\n",
      "2019-01-22 07:40:26,216  tpe.py : INFO  tpe_transform took 0.003987 seconds\n",
      "2019-01-22 07:40:26,217  tpe.py : INFO  TPE using 3/3 trials with best loss 6.679980\n",
      "2019-01-22 07:40:26,220  <ipython-input-8-e876bef2e5bb> : INFO  {'learning_rate': 0.33, 'max_depth': 10.0, 'max_features': 0.35000000000000003, 'n_estimators': 400.0, 'random_state': 2018, 'subsample': 0.6000000000000001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           3.5790           0.6630          293.93m\n",
      "         2           3.2355           0.3057          299.06m\n",
      "         3           3.0491           0.1370          307.87m\n",
      "         4           2.9722           0.0682          293.08m\n",
      "         5           2.8269           0.0316          296.05m\n",
      "         6           2.8336           0.0060          288.40m\n",
      "         7           2.8113           0.0119          284.72m\n",
      "         8           2.7177          -0.0014          282.87m\n",
      "         9           2.7125          -0.0098          285.69m\n",
      "        10           2.6634          -0.0048          274.82m\n",
      "        20           2.4969          -0.0042          224.59m\n",
      "        30           2.4852          -0.0075          202.12m\n",
      "        40           2.3439          -0.0060          177.99m\n",
      "        50           2.3478          -0.0020          166.07m\n",
      "        60           2.2297          -0.0020          162.14m\n",
      "        70           2.1380          -0.0037          156.44m\n",
      "        80           2.1277          -0.0024          147.22m\n",
      "        90           2.0572          -0.0050          142.67m\n",
      "       100           1.9977          -0.0048          138.41m\n"
     ]
    }
   ],
   "source": [
    "skl_min_n_estimators = 10\n",
    "skl_max_n_estimators = 500\n",
    "skl_n_estimators_step = 10\n",
    "skl_n_jobs = 2\n",
    "skl_random_seed = 2018\n",
    "\n",
    "## random forest tree classifier\n",
    "space = {\n",
    "    'n_estimators': hp.quniform(\"n_estimators\", skl_min_n_estimators, skl_max_n_estimators, skl_n_estimators_step),\n",
    "    'learning_rate': hp.quniform(\"learning_rate\", 0.01, 0.5, 0.01),\n",
    "    'max_features': hp.quniform(\"max_features\", 0.05, 1.0, 0.05),\n",
    "    'max_depth': hp.quniform('max_depth', 1, 15, 1),\n",
    "    'subsample': hp.quniform('subsample', 0.5, 1, 0.1),\n",
    "    'random_state': skl_random_seed\n",
    "}\n",
    "\n",
    "def objective(params):\n",
    "    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Tuning\"\"\"\n",
    "    \n",
    "    logger.info(params)\n",
    "    \n",
    "#     gbcf = GradientBoostingClassifier(\n",
    "#         n_estimators=int(params['n_estimators']), \n",
    "#         max_features=params['max_features'], \n",
    "#         learning_rate=params['learning_rate'],\n",
    "#         max_depth=params['max_depth'], \n",
    "#         subsample=params['subsample'],\n",
    "#         random_state=params['random_state']\n",
    "#     )\n",
    "    \n",
    "#     gbcf.fit(train_dataset_x, train_dataset_y)\n",
    "    \n",
    "#     predict_test_prob_y = gbcf.predict_proba(valid_dataset_x)\n",
    "#     model_test_df['Probability'] = predict_test_prob_y[:, 1]\n",
    "    \n",
    "#     score = evaluate(model_test_df)\n",
    "\n",
    "    gbdt = GradientBoostingRegressor(\n",
    "        n_estimators=int(params['n_estimators']), \n",
    "        max_features=params['max_features'], \n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_depth=params['max_depth'], \n",
    "        subsample=params['subsample'],\n",
    "        random_state=params['random_state'],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    gbdt.fit(train_dataset_x, train_dataset_y)\n",
    "    score = gbdt.score(valid_dataset_x, valid_dataset_y)\n",
    "    logging.info('Socre is %f' % score)\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    loss = 1 - score\n",
    "    \n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "\n",
    "def evaluate(result_df):\n",
    "    group = result_df.groupby(['Coupon_id'])\n",
    "    aucs = []\n",
    "    for i in group:\n",
    "        tmpdf = i[1]        \n",
    "        if len(tmpdf['Is_in_day_consume'].unique()) != 2:\n",
    "            continue\n",
    "            \n",
    "        fpr, tpr, thresholds = roc_curve(tmpdf['Is_in_day_consume'], tmpdf['Probability'], pos_label=1)\n",
    "        auc_score = auc(fpr,tpr)\n",
    "        aucs.append(auc_score)\n",
    "            \n",
    "    return np.average(aucs)\n",
    "\n",
    "MAX_EVALS = 500\n",
    "\n",
    "# Optimize\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = Trials())\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>Merchant_id</th>\n",
       "      <th>Coupon_id</th>\n",
       "      <th>Discount_rate</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Date_received</th>\n",
       "      <th>Previous_date_received</th>\n",
       "      <th>Next_date_received</th>\n",
       "      <th>Previous_duration</th>\n",
       "      <th>Next_duration</th>\n",
       "      <th>...</th>\n",
       "      <th>Merchant_distance_receive_count</th>\n",
       "      <th>Merchant_distance_consume_count</th>\n",
       "      <th>Merchant_distance_used_count</th>\n",
       "      <th>Merchant_distance_receive_rate</th>\n",
       "      <th>Merchant_distance_used_rate</th>\n",
       "      <th>User_coupon_duration_used_mean</th>\n",
       "      <th>User_coupon_duration_used_max</th>\n",
       "      <th>User_coupon_duration_used_min</th>\n",
       "      <th>User_received_date_count</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92750</th>\n",
       "      <td>6013165</td>\n",
       "      <td>7963</td>\n",
       "      <td>5548</td>\n",
       "      <td>20:1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20160723</td>\n",
       "      <td>20160722.0</td>\n",
       "      <td>20160728.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92751</th>\n",
       "      <td>6013165</td>\n",
       "      <td>7963</td>\n",
       "      <td>5548</td>\n",
       "      <td>20:1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20160728</td>\n",
       "      <td>20160723.0</td>\n",
       "      <td>20160729.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92748</th>\n",
       "      <td>6013165</td>\n",
       "      <td>7963</td>\n",
       "      <td>5548</td>\n",
       "      <td>20:1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20160721</td>\n",
       "      <td>20160718.0</td>\n",
       "      <td>20160722.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92752</th>\n",
       "      <td>6013165</td>\n",
       "      <td>7963</td>\n",
       "      <td>5548</td>\n",
       "      <td>20:1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20160729</td>\n",
       "      <td>20160728.0</td>\n",
       "      <td>20160731.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92749</th>\n",
       "      <td>6013165</td>\n",
       "      <td>7963</td>\n",
       "      <td>5548</td>\n",
       "      <td>20:1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20160722</td>\n",
       "      <td>20160721.0</td>\n",
       "      <td>20160723.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       User_id  Merchant_id  Coupon_id Discount_rate  Distance  Date_received  \\\n",
       "92750  6013165         7963       5548          20:1       3.0       20160723   \n",
       "92751  6013165         7963       5548          20:1       3.0       20160728   \n",
       "92748  6013165         7963       5548          20:1       3.0       20160721   \n",
       "92752  6013165         7963       5548          20:1       3.0       20160729   \n",
       "92749  6013165         7963       5548          20:1       3.0       20160722   \n",
       "\n",
       "       Previous_date_received  Next_date_received  Previous_duration  \\\n",
       "92750              20160722.0          20160728.0                  2   \n",
       "92751              20160723.0          20160729.0                  6   \n",
       "92748              20160718.0          20160722.0                  4   \n",
       "92752              20160728.0          20160731.0                  2   \n",
       "92749              20160721.0          20160723.0                  2   \n",
       "\n",
       "       Next_duration     ...       Merchant_distance_receive_count  \\\n",
       "92750              6     ...                                   6.0   \n",
       "92751              2     ...                                   6.0   \n",
       "92748              2     ...                                   6.0   \n",
       "92752              3     ...                                   6.0   \n",
       "92749              2     ...                                   6.0   \n",
       "\n",
       "       Merchant_distance_consume_count  Merchant_distance_used_count  \\\n",
       "92750                             10.0                           3.0   \n",
       "92751                             10.0                           3.0   \n",
       "92748                             10.0                           3.0   \n",
       "92752                             10.0                           3.0   \n",
       "92749                             10.0                           3.0   \n",
       "\n",
       "       Merchant_distance_receive_rate  Merchant_distance_used_rate  \\\n",
       "92750                        0.315789                     0.157895   \n",
       "92751                        0.315789                     0.157895   \n",
       "92748                        0.315789                     0.157895   \n",
       "92752                        0.315789                     0.157895   \n",
       "92749                        0.315789                     0.157895   \n",
       "\n",
       "       User_coupon_duration_used_mean  User_coupon_duration_used_max  \\\n",
       "92750                        4.333333                            7.0   \n",
       "92751                        4.333333                            7.0   \n",
       "92748                        4.333333                            7.0   \n",
       "92752                        4.333333                            7.0   \n",
       "92749                        4.333333                            7.0   \n",
       "\n",
       "       User_coupon_duration_used_min  User_received_date_count  Probability  \n",
       "92750                            1.0                         7          1.0  \n",
       "92751                            1.0                         7          1.0  \n",
       "92748                            1.0                         7          1.0  \n",
       "92752                            1.0                         7          1.0  \n",
       "92749                            1.0                         7          1.0  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pred_df = pd.read_csv('lcm_test_features.csv')\n",
    "predict_prob_y = pipe_lr.predict_proba(model_pred_df[fields+continous])\n",
    "model_pred_df['Probability'] = predict_prob_y[:, 1]\n",
    "model_pred_df.sort_values(['Probability'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113640, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result_df = model_pred_df[['User_id', 'Coupon_id', 'Date_received', 'Probability']]\n",
    "final_result_df.to_csv('/Users/leewind/Desktop/submission_20190118.csv', index=False, header=False)\n",
    "final_result_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
