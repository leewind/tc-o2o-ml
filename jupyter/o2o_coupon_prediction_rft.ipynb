{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer, StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, chi2, SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from mlxtend.preprocessing import DenseTransformer\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('ai')\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(asctime)s  %(filename)s : %(levelname)s  %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "+ [Feature transformations with ensembles of trees](https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html)\n",
    "+ [sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "+ [机器学习之 sklearn中的pipeline](http://frankchen.xyz/2018/04/08/pipeline-in-machine-learning/)\n",
    "    - 使用pipeline做cross validation\n",
    "    - 自定义transformer\n",
    "    - FeatureUnion\n",
    "+ [Concatenating multiple feature extraction methods](https://scikit-learn.org/stable/auto_examples/compose/plot_feature_union.html#sphx-glr-auto-examples-compose-plot-feature-union-py)\n",
    "+ [sklearn.pipeline.Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "+ [gbdt+lr demo](https://github.com/princewen/tensorflow_practice/blob/master/recommendation/GBDT%2BLR-Demo/GBDT_LR.py)\n",
    "+ [推荐系统遇上深度学习(十)--GBDT+LR融合方案实战](https://zhuanlan.zhihu.com/p/37522339)\n",
    "+ [python︱sklearn一些小技巧的记录（训练集划分/pipelline/交叉验证等）](https://blog.csdn.net/sinat_26917383/article/details/77917881)\n",
    "+ [16.【进阶】特征提升之特征筛选----feature_selection](https://blog.csdn.net/jh1137921986/article/details/79822512)\n",
    "+ [使用sklearn优雅地进行数据挖掘](https://www.cnblogs.com/jasonfreak/p/5448462.html)\n",
    "+ [Kaggle机器学习之模型融合（stacking）心得](https://zhuanlan.zhihu.com/p/26890738)\n",
    "+ [model_library_config](https://github.com/ChenglongChen/Kaggle_CrowdFlower/blob/master/Code/Model/model_library_config.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../features/lcm_base_features.csv')\n",
    "user_features_df = pd.read_csv('../features/lcm_user_features.csv')\n",
    "merchant_features_df = pd.read_csv('../features/lcm_merchant_features.csv')\n",
    "coupon_features_df = pd.read_csv('../features/lcm_coupon_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipipe = Pipeline([\n",
    "    ('pca', PCA(n_components=2)),\n",
    "    ('scale', MinMaxScaler()),\n",
    "])\n",
    "\n",
    "def get_factor(df, key, prefix):\n",
    "    id_df = df[[key]]\n",
    "    output_df = df.drop([key], axis=1)\n",
    "\n",
    "    ipipe.fit(output_df)\n",
    "    factors = ipipe.transform(output_df)\n",
    "    factors_df = pd.DataFrame(data=factors, columns=[prefix + '_factor_alpha', prefix + '_factor_beta'])\n",
    "    factors_df[key] = id_df[key]\n",
    "    return factors_df\n",
    "\n",
    "df = pd.merge(df, get_factor(user_features_df, 'User_id', 'User'), on=['User_id'], how='left')\n",
    "df = pd.merge(df, get_factor(merchant_features_df, 'Merchant_id', 'Merchant'), on=['Merchant_id'], how='left')\n",
    "df = pd.merge(df, get_factor(coupon_features_df, 'Coupon_id', 'Coupon'), on=['Coupon_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['User_id', 'Merchant_id', 'Coupon_id', 'Distance', 'Date_received',\n",
       "       'Is_in_day_consume', 'Discount', 'Base_consume', 'Discount_money',\n",
       "       'Day_in_month', 'Day_in_week', 'Coupon_type', 'Offline_consume',\n",
       "       'Duration', 'User_factor_alpha', 'User_factor_beta',\n",
       "       'Merchant_factor_alpha', 'Merchant_factor_beta',\n",
       "       'Coupon_factor_alpha', 'Coupon_factor_beta'], dtype=object)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "continous = [\n",
    "    'Discount', \n",
    "    'Base_consume', \n",
    "    'Discount_money',\n",
    "    'User_factor_alpha',\n",
    "    'User_factor_beta',\n",
    "    'Merchant_factor_alpha',\n",
    "    'Merchant_factor_beta',\n",
    "    'Coupon_factor_alpha', \n",
    "    'Coupon_factor_beta'\n",
    "]\n",
    "\n",
    "fields = [\n",
    "    'Distance',\n",
    "    'Day_in_month',\n",
    "    'Day_in_week',\n",
    "    'Coupon_type'\n",
    "]\n",
    "\n",
    "label = ['Is_in_day_consume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_train_df = pd.read_csv('../features/lcm_base_features.csv')\n",
    "# model_train_df = model_train_df[model_train_df['Coupon_id']>0]\n",
    "model_train_df = df[df['Date_received']<20160501]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_test_df = pd.read_csv('../features/lcm_train_test_features.csv')\n",
    "# model_test_df = model_test_df[model_test_df['Coupon_id']>0]\n",
    "model_test_df = model_train_df = df[df['Date_received']>=20160501]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractFeature(TransformerMixin):\n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        return pd.DataFrame(X[:,0] * X[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('continuous', Pipeline(memory=None,\n",
       "     steps=[('extract', ColumnSelector(cols=['Discount', 'Base_consume', 'Discount_money', 'User_factor_alpha', 'User_factor_beta', 'Merchant_factor_alpha', 'Merchant_factor_beta', 'Coupon_factor_alpha', 'Coupon_factor_beta'],\n",
       "        drop_axis=...n='error',\n",
       "       n_values=None, sparse=True)), ('to_dense', DenseTransformer(return_copy=True))]))],\n",
       "       transformer_weights=None))])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('continuous', Pipeline([\n",
    "            ('extract', ColumnSelector(continous)),\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "            ('scale', Normalizer())\n",
    "        ])),\n",
    "        ('fields', Pipeline([\n",
    "            ('extract', ColumnSelector(fields)),\n",
    "            ('imputer', SimpleImputer(missing_values=np.nan,  strategy='most_frequent')),\n",
    "            ('one_hot', OneHotEncoder(categories='auto')),\n",
    "            ('to_dense', DenseTransformer())\n",
    "        ])),\n",
    "    ])),\n",
    "#     ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False))),\n",
    "#     ('skb', SelectKBest(chi2, k=64)),\n",
    "#     ('sc4gbdt', StandardScaler())\n",
    "])\n",
    "\n",
    "fp.fit(model_train_df[fields+continous], model_train_df[label].values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_x = fp.transform(model_train_df[fields+continous])\n",
    "train_dataset_y = model_train_df[label].values.ravel()\n",
    "\n",
    "valid_dataset_x = fp.transform(model_test_df[fields+continous])\n",
    "valid_dataset_y = model_test_df[label].values.ravel()\n",
    "\n",
    "xgbtrain = xgb.DMatrix(train_dataset_x, label=train_dataset_y)\n",
    "xgbvalid = xgb.DMatrix(valid_dataset_x, label=valid_dataset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(result_df):\n",
    "    group = result_df.groupby(['Coupon_id'])\n",
    "    aucs = []\n",
    "    for i in group:\n",
    "        tmpdf = i[1]        \n",
    "        if len(tmpdf['Is_in_day_consume'].unique()) != 2:\n",
    "            continue\n",
    "            \n",
    "        fpr, tpr, thresholds = roc_curve(tmpdf['Is_in_day_consume'], tmpdf['Probability'], pos_label=1)\n",
    "        auc_score = auc(fpr,tpr)\n",
    "        aucs.append(auc_score)\n",
    "            \n",
    "    return np.average(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:00:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 1986 extra nodes, 378 pruned nodes, max_depth=18\n",
      "[454]\ttrain-auc:0.980382\tvalid-auc:0.980382\n",
      "[18:00:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 1808 extra nodes, 330 pruned nodes, max_depth=18\n",
      "[455]\ttrain-auc:0.980384\tvalid-auc:0.980384\n",
      "[18:00:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2830 extra nodes, 530 pruned nodes, max_depth=18\n",
      "[456]\ttrain-auc:0.980396\tvalid-auc:0.980396\n",
      "[18:00:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2068 extra nodes, 304 pruned nodes, max_depth=18\n",
      "[457]\ttrain-auc:0.980408\tvalid-auc:0.980408\n",
      "[18:01:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2852 extra nodes, 498 pruned nodes, max_depth=18\n",
      "[458]\ttrain-auc:0.980426\tvalid-auc:0.980426\n",
      "[18:01:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2786 extra nodes, 526 pruned nodes, max_depth=18\n",
      "[459]\ttrain-auc:0.980429\tvalid-auc:0.980429\n",
      "[18:01:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2746 extra nodes, 522 pruned nodes, max_depth=18\n",
      "[460]\ttrain-auc:0.980439\tvalid-auc:0.980439\n",
      "[18:01:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2146 extra nodes, 472 pruned nodes, max_depth=18\n",
      "[461]\ttrain-auc:0.980448\tvalid-auc:0.980448\n",
      "[18:01:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2536 extra nodes, 444 pruned nodes, max_depth=18\n",
      "[462]\ttrain-auc:0.98046\tvalid-auc:0.98046\n",
      "[18:01:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2382 extra nodes, 472 pruned nodes, max_depth=18\n",
      "[463]\ttrain-auc:0.980461\tvalid-auc:0.980461\n",
      "[18:01:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3076 extra nodes, 596 pruned nodes, max_depth=18\n",
      "[464]\ttrain-auc:0.980476\tvalid-auc:0.980476\n",
      "[18:01:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3092 extra nodes, 554 pruned nodes, max_depth=18\n",
      "[465]\ttrain-auc:0.980488\tvalid-auc:0.980488\n",
      "[18:01:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2082 extra nodes, 364 pruned nodes, max_depth=18\n",
      "[466]\ttrain-auc:0.980484\tvalid-auc:0.980484\n",
      "[18:01:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 1422 extra nodes, 224 pruned nodes, max_depth=18\n",
      "[467]\ttrain-auc:0.98049\tvalid-auc:0.98049\n",
      "[18:01:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 1862 extra nodes, 384 pruned nodes, max_depth=18\n",
      "[468]\ttrain-auc:0.980511\tvalid-auc:0.980511\n",
      "[18:01:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2672 extra nodes, 470 pruned nodes, max_depth=18\n",
      "[469]\ttrain-auc:0.980531\tvalid-auc:0.980531\n",
      "[18:01:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2458 extra nodes, 430 pruned nodes, max_depth=18\n",
      "[470]\ttrain-auc:0.980541\tvalid-auc:0.980541\n",
      "[18:01:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3230 extra nodes, 556 pruned nodes, max_depth=18\n",
      "[471]\ttrain-auc:0.980561\tvalid-auc:0.980561\n",
      "[18:01:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2660 extra nodes, 504 pruned nodes, max_depth=18\n",
      "[472]\ttrain-auc:0.980568\tvalid-auc:0.980568\n",
      "[18:02:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2478 extra nodes, 516 pruned nodes, max_depth=18\n",
      "[473]\ttrain-auc:0.980577\tvalid-auc:0.980577\n",
      "[18:02:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2090 extra nodes, 334 pruned nodes, max_depth=18\n",
      "[474]\ttrain-auc:0.980586\tvalid-auc:0.980586\n",
      "[18:02:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 1868 extra nodes, 320 pruned nodes, max_depth=18\n",
      "[475]\ttrain-auc:0.980592\tvalid-auc:0.980592\n",
      "[18:02:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 1398 extra nodes, 304 pruned nodes, max_depth=18\n",
      "[476]\ttrain-auc:0.980597\tvalid-auc:0.980597\n",
      "[18:02:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2744 extra nodes, 506 pruned nodes, max_depth=18\n",
      "[477]\ttrain-auc:0.980612\tvalid-auc:0.980612\n",
      "[18:02:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2748 extra nodes, 618 pruned nodes, max_depth=18\n",
      "[478]\ttrain-auc:0.980624\tvalid-auc:0.980624\n",
      "[18:02:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2576 extra nodes, 446 pruned nodes, max_depth=18\n",
      "[479]\ttrain-auc:0.980628\tvalid-auc:0.980628\n",
      "[18:02:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2880 extra nodes, 612 pruned nodes, max_depth=18\n",
      "[480]\ttrain-auc:0.980633\tvalid-auc:0.980633\n",
      "[18:02:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2346 extra nodes, 500 pruned nodes, max_depth=18\n",
      "[481]\ttrain-auc:0.980637\tvalid-auc:0.980637\n",
      "[18:02:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2714 extra nodes, 502 pruned nodes, max_depth=18\n",
      "[482]\ttrain-auc:0.980651\tvalid-auc:0.980651\n",
      "[18:02:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 1854 extra nodes, 366 pruned nodes, max_depth=18\n",
      "[483]\ttrain-auc:0.980661\tvalid-auc:0.980661\n",
      "[18:02:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2072 extra nodes, 350 pruned nodes, max_depth=18\n",
      "[484]\ttrain-auc:0.98067\tvalid-auc:0.98067\n",
      "[18:02:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2012 extra nodes, 428 pruned nodes, max_depth=18\n",
      "[485]\ttrain-auc:0.980684\tvalid-auc:0.980684\n",
      "[18:02:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2376 extra nodes, 476 pruned nodes, max_depth=18\n",
      "[486]\ttrain-auc:0.980697\tvalid-auc:0.980697\n",
      "[18:02:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2088 extra nodes, 460 pruned nodes, max_depth=18\n",
      "[487]\ttrain-auc:0.98071\tvalid-auc:0.98071\n",
      "[18:03:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3386 extra nodes, 592 pruned nodes, max_depth=18\n",
      "[488]\ttrain-auc:0.980731\tvalid-auc:0.980731\n",
      "[18:03:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2344 extra nodes, 468 pruned nodes, max_depth=18\n",
      "[489]\ttrain-auc:0.980752\tvalid-auc:0.980752\n",
      "[18:03:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2234 extra nodes, 382 pruned nodes, max_depth=18\n",
      "[490]\ttrain-auc:0.980764\tvalid-auc:0.980764\n",
      "[18:03:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2748 extra nodes, 586 pruned nodes, max_depth=18\n",
      "[491]\ttrain-auc:0.98077\tvalid-auc:0.98077\n",
      "[18:03:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 1866 extra nodes, 368 pruned nodes, max_depth=18\n",
      "[492]\ttrain-auc:0.980781\tvalid-auc:0.980781\n",
      "[18:03:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3368 extra nodes, 634 pruned nodes, max_depth=18\n",
      "[493]\ttrain-auc:0.980796\tvalid-auc:0.980796\n",
      "[18:03:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2604 extra nodes, 442 pruned nodes, max_depth=18\n",
      "[494]\ttrain-auc:0.980803\tvalid-auc:0.980803\n",
      "[18:03:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 1892 extra nodes, 366 pruned nodes, max_depth=18\n",
      "[495]\ttrain-auc:0.980806\tvalid-auc:0.980806\n",
      "[18:03:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3148 extra nodes, 578 pruned nodes, max_depth=18\n",
      "[496]\ttrain-auc:0.98083\tvalid-auc:0.98083\n",
      "[18:03:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2008 extra nodes, 416 pruned nodes, max_depth=18\n",
      "[497]\ttrain-auc:0.980839\tvalid-auc:0.980839\n",
      "[18:03:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2662 extra nodes, 492 pruned nodes, max_depth=18\n",
      "[498]\ttrain-auc:0.980845\tvalid-auc:0.980845\n",
      "[18:03:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 3138 extra nodes, 650 pruned nodes, max_depth=18\n",
      "[499]\ttrain-auc:0.980852\tvalid-auc:0.980852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-26 18:03:43,849  <ipython-input-92-a6d83375ceda> : INFO  train finish\n"
     ]
    }
   ],
   "source": [
    "# model = xgb.sklearn.XGBClassifier(\n",
    "#     nthread=4,\n",
    "#     learn_rate=0.17,\n",
    "#     max_depth=18,\n",
    "#     min_child_weight=1.1,\n",
    "#     subsample=0.7,\n",
    "#     colsample_bytree=0.7,\n",
    "#     colsample_bylevel=0.7,\n",
    "#     objective='rank:pairwise',\n",
    "#     n_estimators=500,\n",
    "#     gamma=0.1,\n",
    "#     reg_alpha=0,\n",
    "#     reg_lambda=1,\n",
    "#     max_delta_step=0,\n",
    "#     scale_pos_weight=1,\n",
    "#     silent=True\n",
    "# )\n",
    "\n",
    "params = {\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'rank:pairwise',\n",
    "    'eval_metric': 'auc',\n",
    "    'gamma': 0.1,\n",
    "    'min_child_weight': 1.1,\n",
    "    'max_depth': 18,\n",
    "    'lambda': 1,\n",
    "    'alpha': 0,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'colsample_bylevel': 0.7,\n",
    "    'eta': 0.17,\n",
    "    'tree_method': 'exact',\n",
    "    'seed': 2018,\n",
    "    'nthread': 20\n",
    "}\n",
    "\n",
    "watchlist = [(xgbtrain, 'train'), (xgbvalid, 'valid')]\n",
    "\n",
    "logging.info('train begin')\n",
    "model = xgb.train(params, xgbtrain, num_boost_round=500, evals=watchlist, early_stopping_rounds=10)\n",
    "logging.info('train finish')\n",
    "\n",
    "model.save_model('../model/xgb.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_result(result):\n",
    "    return MinMaxScaler(copy=True, feature_range=(0, 1)).fit_transform(result.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leewind/.local/share/virtualenvs/leewind-p6XO93Th/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9405666423255918"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_test_prob_raw = model.predict(xgbvalid)\n",
    "predict_test_prob = transfer_result(predict_test_prob_raw)\n",
    "model_test_df['Probability'] = predict_test_prob\n",
    "evaluate(model_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    306313.000000\n",
       "mean          0.418945\n",
       "std           0.142300\n",
       "min           0.000000\n",
       "25%           0.323448\n",
       "50%           0.419343\n",
       "75%           0.514778\n",
       "max           1.000000\n",
       "Name: Probability, dtype: float64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test_df['Probability'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred_df = pd.read_csv('../features/lcm_submit_features.csv')\n",
    "model_pred_df = pd.merge(model_pred_df, get_factor(user_features_df, 'User_id', 'User'), on=['User_id'], how='left')\n",
    "model_pred_df = pd.merge(model_pred_df, get_factor(merchant_features_df, 'Merchant_id', 'Merchant'), on=['Merchant_id'], how='left')\n",
    "model_pred_df = pd.merge(model_pred_df, get_factor(coupon_features_df, 'Coupon_id', 'Coupon'), on=['Coupon_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>Merchant_id</th>\n",
       "      <th>Coupon_id</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Date_received</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Base_consume</th>\n",
       "      <th>Discount_money</th>\n",
       "      <th>Day_in_month</th>\n",
       "      <th>Day_in_week</th>\n",
       "      <th>Coupon_type</th>\n",
       "      <th>User_factor_alpha</th>\n",
       "      <th>User_factor_beta</th>\n",
       "      <th>Merchant_factor_alpha</th>\n",
       "      <th>Merchant_factor_beta</th>\n",
       "      <th>Coupon_factor_alpha</th>\n",
       "      <th>Coupon_factor_beta</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43671</th>\n",
       "      <td>2751537</td>\n",
       "      <td>7910.0</td>\n",
       "      <td>2637.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20160702.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.395742</td>\n",
       "      <td>0.057898</td>\n",
       "      <td>0.006350</td>\n",
       "      <td>0.329149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43669</th>\n",
       "      <td>2751537</td>\n",
       "      <td>7910.0</td>\n",
       "      <td>2637.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20160702.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.395742</td>\n",
       "      <td>0.057898</td>\n",
       "      <td>0.006350</td>\n",
       "      <td>0.329149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42251</th>\n",
       "      <td>6139850</td>\n",
       "      <td>6135.0</td>\n",
       "      <td>8182.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20160715.0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.141453</td>\n",
       "      <td>0.080237</td>\n",
       "      <td>0.006191</td>\n",
       "      <td>0.330748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.900718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45934</th>\n",
       "      <td>3977895</td>\n",
       "      <td>4808.0</td>\n",
       "      <td>1226.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20160709.0</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.050597</td>\n",
       "      <td>0.090049</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.314645</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.895087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78008</th>\n",
       "      <td>3229547</td>\n",
       "      <td>6189.0</td>\n",
       "      <td>12807.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20160712.0</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.050586</td>\n",
       "      <td>0.088519</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>0.317547</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.882158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       User_id  Merchant_id  Coupon_id  Distance  Date_received  Discount  \\\n",
       "43671  2751537       7910.0     2637.0       0.0     20160702.0  0.833333   \n",
       "43669  2751537       7910.0     2637.0       0.0     20160702.0  0.833333   \n",
       "42251  6139850       6135.0     8182.0       0.0     20160715.0  0.900000   \n",
       "45934  3977895       4808.0     1226.0       0.0     20160709.0  0.950000   \n",
       "78008  3229547       6189.0    12807.0       0.0     20160712.0  0.966667   \n",
       "\n",
       "       Base_consume  Discount_money  Day_in_month  Day_in_week  Coupon_type  \\\n",
       "43671          30.0             5.0           2.0          6.0          1.0   \n",
       "43669          30.0             5.0           2.0          6.0          1.0   \n",
       "42251          10.0             1.0          15.0          5.0          1.0   \n",
       "45934          20.0             1.0           9.0          6.0          1.0   \n",
       "78008          30.0             1.0          12.0          2.0          1.0   \n",
       "\n",
       "       User_factor_alpha  User_factor_beta  Merchant_factor_alpha  \\\n",
       "43671           0.395742          0.057898               0.006350   \n",
       "43669           0.395742          0.057898               0.006350   \n",
       "42251           0.141453          0.080237               0.006191   \n",
       "45934           0.050597          0.090049               0.000071   \n",
       "78008           0.050586          0.088519               0.001129   \n",
       "\n",
       "       Merchant_factor_beta  Coupon_factor_alpha  Coupon_factor_beta  \\\n",
       "43671              0.329149                  NaN                 NaN   \n",
       "43669              0.329149                  NaN                 NaN   \n",
       "42251              0.330748                  NaN                 NaN   \n",
       "45934              0.314645                  NaN                 NaN   \n",
       "78008              0.317547                  NaN                 NaN   \n",
       "\n",
       "       Probability  \n",
       "43671     1.000000  \n",
       "43669     1.000000  \n",
       "42251     0.900718  \n",
       "45934     0.895087  \n",
       "78008     0.882158  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_dataset_x = fp.transform(model_pred_df[fields+continous])\n",
    "xgbPredict = xgb.DMatrix(predict_dataset_x)\n",
    "predict_prob_raw = model.predict(xgbPredict)\n",
    "model_pred_df['Probability'] = transfer_result(predict_prob_raw)\n",
    "model_pred_df.sort_values(['Probability'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113640, 4)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result_df = model_pred_df[['User_id', 'Coupon_id', 'Date_received', 'Probability']]\n",
    "final_result_df.to_csv('/Users/leewind/Desktop/submission_20190127_2.csv', index=False, header=False)\n",
    "final_result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>Coupon_id</th>\n",
       "      <th>Date_received</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.136400e+05</td>\n",
       "      <td>113640.000000</td>\n",
       "      <td>1.136400e+05</td>\n",
       "      <td>113640.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.684858e+06</td>\n",
       "      <td>9053.810929</td>\n",
       "      <td>2.016072e+07</td>\n",
       "      <td>0.421957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.126259e+06</td>\n",
       "      <td>4145.873088</td>\n",
       "      <td>9.019508e+00</td>\n",
       "      <td>0.115079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.090000e+02</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.016070e+07</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.844191e+06</td>\n",
       "      <td>5023.000000</td>\n",
       "      <td>2.016071e+07</td>\n",
       "      <td>0.345981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.683266e+06</td>\n",
       "      <td>9983.000000</td>\n",
       "      <td>2.016072e+07</td>\n",
       "      <td>0.423869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.525845e+06</td>\n",
       "      <td>13602.000000</td>\n",
       "      <td>2.016072e+07</td>\n",
       "      <td>0.504204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.361024e+06</td>\n",
       "      <td>14045.000000</td>\n",
       "      <td>2.016073e+07</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            User_id      Coupon_id  Date_received    Probability\n",
       "count  1.136400e+05  113640.000000   1.136400e+05  113640.000000\n",
       "mean   3.684858e+06    9053.810929   2.016072e+07       0.421957\n",
       "std    2.126259e+06    4145.873088   9.019508e+00       0.115079\n",
       "min    2.090000e+02       3.000000   2.016070e+07       0.000000\n",
       "25%    1.844191e+06    5023.000000   2.016071e+07       0.345981\n",
       "50%    3.683266e+06    9983.000000   2.016072e+07       0.423869\n",
       "75%    5.525845e+06   13602.000000   2.016072e+07       0.504204\n",
       "max    7.361024e+06   14045.000000   2.016073e+07       1.000000"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result_df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
